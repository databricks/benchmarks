<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Utils - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta name="robots" content="nofollow">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableNotebookNotifications":true,"enableSshKeyUI":false,"defaultInteractivePricePerDBU":0.4,"enableClusterMetricsUI":true,"useReactTableCreateView":false,"enableOnDemandClusterType":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","enableJobsPrefetching":true,"workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/index.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableClearStateFeature":true,"enableJobsAclsV2InUI":false,"dbcForumURL":"http://forums.databricks.com/","enableProtoClusterInfoDeltaPublisher":true,"enableAttachExistingCluster":true,"resetJobListOnConnect":true,"serverlessDefaultSparkVersion":"latest-stable-scala2.11","maxCustomTags":45,"serverlessDefaultMaxWorkers":20,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"support_ssh":false,"spark_heap_memory":4800,"instance_type_id":"r3.2xlarge","spark_core_oversubscription_factor":8.0,"node_type_id":"dev-tier-node","description":"Community Optimized","support_cluster_tags":false,"container_memory_mb":6000,"node_instance_type":{"instance_type_id":"r3.2xlarge","provider":"AWS","local_disk_size_gb":160,"compute_units":26.0,"number_of_ips":14,"local_disks":1,"reserved_compute_units":3.64,"gpus":0,"memory_mb":62464,"num_cores":8,"local_disk_type":"AHCI","max_attachable_disks":0,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":6144,"is_hidden":false,"category":"Community Edition","num_cores":0.88,"support_port_forwarding":false,"support_ebs_volumes":false,"is_deprecated":false}],"default_node_type_id":"dev-tier-node"},"sqlAclsDisabledMap":{"spark.databricks.acl.enabled":"false","spark.databricks.acl.sqlOnly":"false"},"enableDatabaseSupportClusterChoice":true,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":999999,"serverlessClusterProductName":"Serverless Pool","showS3TableImportOption":true,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"timerUpdateQueueLength":100,"sqlAclsEnabledMap":{"spark.databricks.acl.enabled":"true","spark.databricks.acl.sqlOnly":"true"},"enableLargeResultDownload":true,"maxElasticDiskCapacityGB":5000,"serverlessDefaultMinWorkers":2,"zoneInfos":[{"id":"us-west-2c","isDefault":true},{"id":"us-west-2b","isDefault":false},{"id":"us-west-2a","isDefault":false}],"enableCustomSpotPricingUIByTier":false,"serverlessClustersEnabled":false,"enableFindAndReplace":true,"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":true,"enableBitbucketCloud":true,"enableMaxConcurrentRuns":true,"enableJobAclsConfig":false,"enableFullTextSearch":false,"enableElasticSparkUI":false,"enableNewClustersCreate":true,"clusters":true,"allowRunOnPendingClusters":true,"useAutoscalingByDefault":false,"enableAzureToolbar":false,"fileStoreBase":"FileStore","enableEmailInAzure":false,"enableRLibraries":true,"enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":true,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":false,"checkBeforeAddingAadUser":false,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"perClusterAutoterminationEnabled":false,"enableNotebookCommandNumbers":true,"sparkVersions":[{"key":"1.6.3-db2-hadoop2-scala2.10","displayName":"Spark 1.6.3-db2 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-aba860a0ffce4f3471fb14aefdcb1d768ac66a53a5ad884c48745ef98aeb9d67","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"3.3.x-gpu-scala2.11","displayName":"3.3 (includes Apache Spark 2.2.0, GPU, Scala 2.11)","packageLabel":"spark-image-280a8d41cd338f5b48d43eb87622c542c6e6584c430f6d3afe8f3401b9607cb9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db5-scala2.11","displayName":"Spark 2.1.1-db5 (Scala 2.11)","packageLabel":"spark-image-08d9fc1551087e0876236f19640c4a83116b1649f15137427d21c9056656e80e","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.3.x-scala2.10","displayName":"3.3 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-dd410c68e21c3c563ad6128d35705b605d70530124d55aff1dd12d7e15adfa20","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1, deprecated)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.2.x-scala2.11","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-67ab3a06d1e83d5b60df7063245eb419a2e9fe329aeeb7e7d9713332c669bb17","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.1-db6-scala2.10","displayName":"Spark 2.1.1-db6 (Scala 2.10)","packageLabel":"spark-image-177f3f02a6a3432d30068332dc857b9161345bdd2ee8a2d2de05bb05cb4b0f4c","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.11","displayName":"Spark 2.1.0-db2 (Scala 2.11)","packageLabel":"spark-image-267c4490a3ab8a39acdbbd9f1d36f6decdecebf013e30dd677faff50f1d9cf8b","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.x-gpu-scala2.11","displayName":"Spark 2.1 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-d613235f93e0f29838beb2079a958c02a192ed67a502192bc67a8a5f2fb37f35","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"latest-stable-gpu-scala2.11","displayName":"Latest stable (3.3, GPU, Scala 2.11)","packageLabel":"spark-image-280a8d41cd338f5b48d43eb87622c542c6e6584c430f6d3afe8f3401b9607cb9","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db3-scala2.10","displayName":"Spark 2.0.2-db3 (Scala 2.10)","packageLabel":"spark-image-584091dedb690de20e8cf22d9e02fdcce1281edda99eedb441a418d50e28088f","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.2.x-scala2.10","displayName":"3.2 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-557788bea0eea16bbf7a8ba13ace07e64dd7fc86270bd5cea086097fe886431f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"latest-experimental-scala2.10","displayName":"Latest experimental (3.4 snapshot, Scala 2.10)","packageLabel":"spark-image-8f0a6f29ad4fa8868b77f325774f0d97423ecc9b2d45b3716772ef91b4464857","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0-db1 (Scala 2.11)","packageLabel":"spark-image-e8ad5b72cf0f899dcf2b4720c1f572ab0e87a311d6113b943b4e1d4a7edb77eb","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.1-db4-scala2.11","displayName":"Spark 2.1.1-db4 (Scala 2.11)","packageLabel":"spark-image-52bca0ca866e3f4243d3820a783abf3b9b3b553edf234abef14b892657ceaca9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"latest-rc-scala2.11","displayName":"Latest RC (3.4, Scala 2.11)","packageLabel":"spark-image-e73f9cc730e88d7b9175afaf35b226bbfec5b36b873cd0462902e178720b6a0e","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"latest-stable-scala2.11","displayName":"Latest stable (3.3, Scala 2.11)","packageLabel":"spark-image-73a161da0570b3f51c8eb238602af2f5561789ea80b25c69a48691fc84e2d974","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.0-db2-scala2.10","displayName":"Spark 2.1.0-db2 (Scala 2.10)","packageLabel":"spark-image-a2ca4f6b58c95f78dca91b1340305ab3fe32673bd894da2fa8e1dc8a9f8d0478","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db4-scala2.11","displayName":"Spark 2.0.2-db4 (Scala 2.11)","packageLabel":"spark-image-7dbc7583e8271765b8a1508cb9e832768e35489bbde2c4c790bc6766aee2fd7f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-968b89f1d0ec32e1ee4dacd04838cae25ef44370a441224177a37980d539d83a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-eaa8d9b990015a14e032fb2e2e15be0b8d5af9627cd01d855df728b67969d5d9","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.3-db2-hadoop1-scala2.10","displayName":"Spark 1.6.3-db2 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-14112ea0645bea94333a571a150819ce85573cf5541167d905b7e6588645cf3b","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-36d48f22cca7a907538e07df71847dd22aaf84a852c2eeea2dcefe24c681602f","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-8e1c50d626a52eac5a6c8129e09ae206ba9890f4523775f77af4ad6d99a64c44","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-859e88079f97f58d50e25163b39a1943d1eeac0b6939c5a65faba986477e311a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db4-scala2.10","displayName":"Spark 2.1.1-db4 (Scala 2.10)","packageLabel":"spark-image-c7c0224de396cd1563addc1ae4bca6ba823780b6babe6c3729ddf73008f29ba4","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"latest-rc-scala2.10","displayName":"Latest RC (3.4, Scala 2.10)","packageLabel":"spark-image-8f0a6f29ad4fa8868b77f325774f0d97423ecc9b2d45b3716772ef91b4464857","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"latest-stable-scala2.10","displayName":"Latest stable (3.3, Scala 2.10)","packageLabel":"spark-image-dd410c68e21c3c563ad6128d35705b605d70530124d55aff1dd12d7e15adfa20","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-c2d623f03dd44097493c01aa54a941fc31978ebe6d759b36c75b716b2ff6ab9c","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db4-scala2.10","displayName":"Spark 2.0.2-db4 (Scala 2.10)","packageLabel":"spark-image-859e88079f97f58d50e25163b39a1943d1eeac0b6939c5a65faba986477e311a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db5-scala2.10","displayName":"Spark 2.1.1-db5 (Scala 2.10)","packageLabel":"spark-image-74133df2c13950431298d1cab3e865c191d83ac33648a8590495c52fc644c654","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1, deprecated)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"latest-experimental-gpu-scala2.11","displayName":"Latest experimental (3.4 snapshot, GPU, Scala 2.11)","packageLabel":"spark-image-1faca7b63a0952e44bb249657efae7b655a85c1480f43978398a6ea7104e20bc","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.2.x-scala2.10","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-d549f2d4a523994ecdf37e531b51d5ec7d8be51534bb0ca5322eaad28ba8f557","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.0.x-scala2.11","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-67ab3a06d1e83d5b60df7063245eb419a2e9fe329aeeb7e7d9713332c669bb17","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-7dbc7583e8271765b8a1508cb9e832768e35489bbde2c4c790bc6766aee2fd7f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-177f3f02a6a3432d30068332dc857b9161345bdd2ee8a2d2de05bb05cb4b0f4c","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"3.1.x-scala2.11","displayName":"3.1 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-241fa8b78ee6343242b1756b18076270894385ff40a81172a6fb5eadf66155d3","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.0-db3-scala2.10","displayName":"Spark 2.1.0-db3 (Scala 2.10)","packageLabel":"spark-image-25a17d070af155f10c4232dcc6248e36a2eb48c24f8d4fc00f34041b86bd1626","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-4fa852ba378e97815083b96c9cada7b962a513ec23554a5fc849f7f1dd8c065a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"3.1.x-scala2.10","displayName":"3.1 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-7efac6b9a8f2da59cb4f6d0caac46cfcb3f1ebf64c8073498c42d0360f846714","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.3.x-scala2.11","displayName":"3.3 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-73a161da0570b3f51c8eb238602af2f5561789ea80b25c69a48691fc84e2d974","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1, deprecated)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db3-scala2.11","displayName":"Spark 2.0.2-db3 (Scala 2.11)","packageLabel":"spark-image-7fd7aaa89d55692e429115ae7eac3b1a1dc4de705d50510995f34306b39c2397","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.1-db6-scala2.11","displayName":"Spark 2.1.1-db6 (Scala 2.11)","packageLabel":"spark-image-fdad9ef557700d7a8b6bde86feccbcc3c71d1acdc838b0fd299bd19956b1076e","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-d50af1032799546b8ccbeeb76889a20c819ebc2a0e68ea20920cb30d3895d3ae","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-654bdd6e9bad70079491987d853b4b7abf3b736fff099701501acaabe0e75c41","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-a659f3909d51b38d297b20532fc807ecf708cfb7440ce9b090c406ab0c1e4b7e","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"latest-experimental-scala2.11","displayName":"Latest experimental (3.4 snapshot, Scala 2.11)","packageLabel":"spark-image-e73f9cc730e88d7b9175afaf35b226bbfec5b36b873cd0462902e178720b6a0e","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.2.x-scala2.11","displayName":"3.2 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-5537926238bc55cb6cd76ee0f0789511349abead3781c4780721a845f34b5d4e","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.x-scala2.11","displayName":"Spark 2.1 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-fdad9ef557700d7a8b6bde86feccbcc3c71d1acdc838b0fd299bd19956b1076e","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0-db1 (Scala 2.10)","packageLabel":"spark-image-f0ab82a5deb7908e0d159e9af066ba05fb56e1edb35bdad41b7ad2fd62a9b546","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"3.0.x-scala2.10","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-d549f2d4a523994ecdf37e531b51d5ec7d8be51534bb0ca5322eaad28ba8f557","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.0-db3-scala2.11","displayName":"Spark 2.1.0-db3 (Scala 2.11)","packageLabel":"spark-image-ccbc6b73f158e2001fc1fb8c827bfdde425d8bd6d65cb7b3269784c28bb72c16","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"latest-rc-gpu-scala2.11","displayName":"Latest RC (3.4 GPU, Scala 2.11)","packageLabel":"spark-image-1faca7b63a0952e44bb249657efae7b655a85c1480f43978398a6ea7104e20bc","upgradable":true,"deprecated":false,"customerVisible":false}],"enablePresentationMode":false,"enableClearStateAndRunAll":true,"enableRestrictedClusterCreation":true,"enableFeedback":true,"enableClusterAutoScaling":false,"enableUserVisibleDefaultTags":true,"defaultNumWorkers":0,"serverContinuationTimeoutMillis":10000,"jobsUnreachableThresholdMillis":60000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":false,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","showDbuPricing":true,"databricksDocsBaseHostname":"docs.databricks.com","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"i3.4xlarge":4,"class-node":1,"m4.2xlarge":1.5,"r4.xlarge":1,"m4.4xlarge":3,"r4.16xlarge":16,"Standard_DS11":0.5,"p2.8xlarge":16,"m4.10xlarge":8,"r3.8xlarge":8,"r4.4xlarge":4,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"m4.xlarge":0.75,"r4.8xlarge":8,"r4.large":0.5,"Standard_DS12":1,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"i3.large":0.75,"memory-optimized":1,"m4.large":0.375,"p2.16xlarge":24,"i3.8xlarge":8,"i3.16xlarge":16,"Standard_DS12_v2":1,"Standard_DS13":2,"Standard_DS11_v2":0.5,"Standard_DS13_v2":2,"c3.2xlarge":1,"Standard_L4s":1.5,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"i3.2xlarge":2,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"m4.16xlarge":12,"c4.8xlarge":4,"i3.xlarge":1,"r3.xlarge":1,"r4.2xlarge":2,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":false,"metastoreServiceRowLimit":1000000,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":true,"enableClusterTagsUI":false,"enableNotebookHistoryDiffing":true,"branch":"2.55","accountsLimit":3,"enableSparkEnvironmentVariables":true,"enableX509Authentication":false,"useAADLogin":false,"enableStructuredStreamingNbOptimizations":true,"enableNotebookGitBranching":true,"local":false,"enableNotebookLazyRenderWrapper":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"showReleaseNote":true,"displayDefaultContainerMemoryGB":6,"enableNotebookCommandMode":true,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"removePasswordInAccountSettings":false,"preferStartTerminatedCluster":false,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"sandboxForUrlSandboxFrame":"allow-scripts allow-popups allow-popups-to-escape-sandbox allow-forms","enableCssTransitions":true,"serverlessEnableElasticDisk":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":false,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableSshKeyUIByTier":false,"enableCreateClusterOnAttach":true,"defaultAutomatedPricePerDBU":0.2,"enableNotebookGitVersioning":true,"defaultMinWorkers":2,"files":"files/","feedbackEmail":"feedback@databricks.com","enableDriverLogsUI":true,"defaultMaxWorkers":8,"enableWorkspaceAclsConfig":false,"dropzoneMaxFileSize":2047,"enableNewClustersList":true,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"enableSparkEnvironmentVariablesUI":false,"defaultSparkVersion":{"key":"3.2.x-scala2.11","displayName":"3.2 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-5537926238bc55cb6cd76ee0f0789511349abead3781c4780721a845f34b5d4e","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":false,"enableMountAclsConfig":false,"defaultAutoterminationMin":180,"useDevTierHomePage":true,"enableClusterClone":true,"enableNotebookLineNumbers":true,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":false,"enableNotebookDatasetInfoView":true,"enableClusterAclsByTier":false,"databricksDocsBaseUrl":"https://docs.databricks.com/","azurePortalLink":"https://portal.azure.com","cloud":"AWS","disallowAddingAdmins":true,"enableSparkConfUI":true,"featureTier":"DEVELOPER_BASIC_TIER","mavenCentralSearchEndpoint":"http://search.maven.org/solrsearch/select","enableOrgSwitcherUI":true,"bitbucketCloudBaseApiV2Url":"https://api.bitbucket.org/2.0","clustersLimit":1,"enableJdbcImport":true,"enableElasticDisk":false,"logfiles":"logfiles/","enableRelativeNotebookLinks":true,"enableMultiSelect":true,"enableWebappSharding":true,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"separateTableForJobClusters":true,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"dbcFeedbackURL":"mailto:feedback@databricks.com","enableMountAclService":true,"enableStructuredDataAcls":false,"showVersion":true,"serverlessClustersByDefault":false,"enableWorkspaceAcls":false,"maxClusterTagKeyLength":127,"gitHash":"7e487e8906bd141e3032c363a823d3c48d1535b5","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","serverlessAttachEbsVolumesByDefault":false,"enableTokensConfig":false,"allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableTokens":false,"enableMiniClusters":true,"enableNewJobList":true,"enableDebugUI":false,"enableStreamingMetricsDashboard":true,"allowNonAdminUsers":true,"enableSingleSignOnByTier":false,"enableJobsRetryOnTimeout":true,"useStandardTierUpgradeTooltips":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSpotClusterType":true,"enableSparkPackages":true,"checkAadUserInWorkspaceTenant":false,"dynamicSparkVersions":true,"enableClusterTagsUIByTier":false,"enableNotebookHistoryUI":true,"enableClusterLoggingUI":true,"enableDatabaseDropdownInTableUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":false,"enableFolderHtmlExport":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.databricks.com/_static/notebooks/gentle-introduction-to-apache-spark.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":"img/home/Python_icon.svg"}],"enableClusterStart":false,"enableEBSVolumesUIByTier":false,"singleSignOnComingSoon":false,"removeSubCommandCodeWhenExport":true,"upgradeURL":"https://accounts.cloud.databricks.com/registration.html#login","maxAutoterminationMinutes":10000,"autoterminateClustersByDefault":true,"notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":false,"showForgotPasswordLink":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"minAutoterminationMinutes":10,"accounts":true,"useOnDemandClustersByDefault":true,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"enableAutoCreateUserUI":true,"defaultCoresPerContainer":4,"showTerminationReason":true,"enableNewClustersGet":true,"showPricePerDBU":false,"showSqlProxyUI":true,"enableNotebookErrorHighlighting":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":2044923475617469,"name":"Utils","language":"scala","commands":[{"version":"CommandV1","origId":2044923475617471,"guid":"bea18ce4-36ea-4a20-ae65-67120af04898","subtype":"command","commandType":"auto","position":0.125,"command":"package com.databricks.spark\n\nimport java.io.File\nimport java.nio.file.{Files, Path, Paths, StandardCopyOption}\nimport java.nio.file.attribute.PosixFilePermissions\nimport java.text.SimpleDateFormat\nimport java.util.{Date, Properties}\n\nimport scala.sys.process._\n\nimport org.apache.spark.SparkContext\n\n/** Utility functions for being able to execute shell commands on executors using ssh. */\ntrait SSHUtils {\n  \n  val sc: SparkContext\n  \n  private def writeFile(path: String, contents: String, append: Boolean = false): Unit = {\n    val fw = new java.io.FileWriter(path, append)\n    fw.write(contents)\n    fw.close()\n  }\n  \n  lazy val publicKey = \"cat /root/.ssh/id_rsa.pub\".!!\n\n  /**\n   * Inject private key into executors so that the driver can ssh into them.\n   */\n  def addAuthorizedPublicKey(key: String): Unit = {\n    writeFile(\"/home/ubuntu/.ssh/authorized_keys\", \"\\n\" + key, true)\n  }\n\n  /**\n   * Ssh into the given `host` and execute `command`.\n   */\n  def ssh(host: String, command: String, logStdout: Boolean = true): String = {\n    println(\"executing command - \" + command + \" on host: \" + host)\n    val outBuffer = new collection.mutable.ArrayBuffer[String]()\n    val logger = ProcessLogger(line => outBuffer += line, println(_))\n\n    val exitCode = \n      Seq(\"ssh\", \"-o\", \"StrictHostKeyChecking=no\", \"-p\", \"22\", \"-i\", \"/root/.ssh/id_rsa\", s\"ubuntu@$host\", s\"$command\") ! logger\n    if (logStdout) {\n      outBuffer.foreach(println)\n    }\n    if (exitCode != 0) {\n      println(s\"FAILED: command - $command on host: $host\")\n      sys.error(\"Command failed\")\n    }\n    println(s\"SUCCESS: command - $command on host: $host\")\n    outBuffer.mkString(\"\\n\")\n  }\n  \n  /** Distribute the public key on executors as `authorized_keys`. */\n  protected def setupSSH(numExecutors: Int): Unit = {\n    val key = publicKey\n    sc.parallelize(0 until numExecutors, numExecutors).foreach { i =>\n      addAuthorizedPublicKey(key)\n    }\n  }\n  \n  /** Generate new ssh keys if required. */\n  protected def generateSshKeys(): Unit = {\n    if (\"ls /root/.ssh/id_rsa\".! > 0) {\n      Seq(\"ssh-keygen\", \"-t\" , \"rsa\", \"-N\", \"\", \"-f\", \"/root/.ssh/id_rsa\").!!\n    }\n  }\n}","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1497563459163,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6e29f404-c20d-4460-95fa-9fc487709dbc"},{"version":"CommandV1","origId":2044923475617472,"guid":"22f40a2a-81e4-4a59-8816-7cb974cefecd","subtype":"command","commandType":"auto","position":0.25,"command":"package com.databricks.spark\n\nimport java.io.File\nimport java.nio.file.{Files, Path, Paths, StandardCopyOption}\nimport java.nio.file.attribute.PosixFilePermissions\nimport java.text.SimpleDateFormat\nimport java.util.{Date, Properties}\n\nimport org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}\n\nimport org.apache.spark.sql._\n\nimport sys.process._\n\n/**\n * Class for managing Apache Kafka on a Databricks cluster. Currently supports one broker per EC2 instance.\n * Will install Zookeeper on a single node, colocated with the first broker.\n * Please do not use this code in production.\n *\n * @param spark SparkSession that will allow us to run Spark jobs to distribute ssh public keys to executors\n * @param numKafkaNodes Number of Kafka brokers to install. Must be smaller than or equal to the number of executors\n * @param stopSparkOnKafkaNodes Whether to kill Spark executors on the instances we install Kafka for better isolation\n * @param kafkaVersion The version of Kafka to install\n */\nclass LocalKafka(\n    spark: SparkSession,\n    numKafkaNodes: Int = 1,\n    stopSparkOnKafkaNodes: Boolean = false,\n    kafkaVersion: String = \"0.10.2.1\") extends Serializable with SSHUtils {\n  import spark.implicits._\n  @transient val sc = spark.sparkContext\n  \n  import LocalKafka._\n  \n  private val workers: List[String] = {\n    val executors = sc.getExecutorMemoryStatus.keys.map(_.split(\":\").head).toSet\n    if (executors.size == 1) {\n      // just the driver\n      List(myIp)\n    } else {\n      (executors - myIp).toList\n    }\n  }\n  private val numExecutors = workers.length\n  \n  require(numExecutors >= numKafkaNodes,\n    s\"\"\"You don't have enough executors to maintain $numKafkaNodes Kafka brokers. Please increase your cluster size,\n       |or decrease the amount of Kafka brokers. Available executors: $numExecutors.\n     \"\"\".stripMargin)\n  if (stopSparkOnKafkaNodes) {\n    require(List(myIp) != workers, \"You can't stop Spark when running Kafka only on the driver.\")\n  }\n  lazy val kafkaNodes = workers.take(numKafkaNodes)\n  private lazy val zookeeper = workers(0)\n  lazy val kafkaNodesString = kafkaNodes.map(_ + \":9092\").mkString(\",\")\n  lazy val zookeeperAddress = zookeeper + \":2181\"\n  private val dbfsDir = \"home/streaming/benchmark\"\n  \n  def init(): Unit = {\n    generateSshKeys()\n    writeInstallFile(dbfsDir, kafkaVersion)\n  }\n  \n  init()\n\n  /**\n   * Setup Kafka in cluster\n   */\n  def setup() = {\n    setupSSH(numExecutors)\n\n    workers.foreach { ip =>\n      ssh(ip, s\"bash /dbfs/$dbfsDir/install-kafka.sh\")\n    }\n\n    ssh(zookeeper, s\"kafka/bin/zookeeper-server-start.sh -daemon kafka/config/zookeeper.properties\")\n\n    kafkaNodes.zipWithIndex.foreach{ case (host, id) =>\n      ssh(host, s\"bash /dbfs/$dbfsDir/configure-kafka.sh $zookeeper $id $host\")\n      ssh(host, s\"kafka/bin/kafka-server-start.sh -daemon kafka/config/server.properties\")\n    }\n\n    Thread.sleep(30 * 1000) // wait for Kafka to come up\n\n    if (stopSparkOnKafkaNodes) {\n      kafkaNodes.foreach { ip =>\n        ssh(ip, s\"sudo monit stop spark-slave\")\n      }\n    }\n  }\n\n  /** Create a `topic` on Kafka with the given number of partitions and replication factor. */\n  def createTopic(topic: String, partitions: Int = 8, replFactor: Int = 1): Unit = {\n    ssh(kafkaNodes(0), s\"kafka/bin/kafka-topics.sh --create --topic $topic --partitions $partitions \" +\n      s\"--replication-factor $replFactor --zookeeper $zookeeper:2181\")\n  }\n  \n  /** Delete the given topic if it exists. */\n  def deleteTopicIfExists(topic: String): Unit = {\n    try {\n      ssh(kafkaNodes(0), s\"kafka/bin/kafka-topics.sh --delete --topic $topic --zookeeper $zookeeper:2181\")\n    } catch {\n      case e: RuntimeException =>\n    }\n  }\n}\n\nobject LocalKafka extends Serializable {\n  var cluster: LocalKafka = null\n  def setup(spark: SparkSession, numKafkaNodes: Int = 1, stopSparkOnKafkaNodes: Boolean = false): LocalKafka = {\n    if (cluster == null) {\n      cluster = new LocalKafka(spark, numKafkaNodes, stopSparkOnKafkaNodes)\n      cluster.setup()\n    }\n    cluster\n  }\n  \n  private def myIp: String = {\n    \"hostname\".!!.split(\"_\").takeRight(4).map(_.trim).mkString(\".\")\n  }\n  \n  private def writeFile(path: String, contents: String, append: Boolean = false): Unit = {\n    val fw = new java.io.FileWriter(path, append)\n    fw.write(contents)\n    fw.close()\n  }\n  \n  private def writeInstallFile(dbfsDir: String, kafkaVersion: String): Unit = {\n    Seq(\"mkdir\", \"-p\", s\"/dbfs/$dbfsDir\").!!\n    writeFile(s\"/dbfs/$dbfsDir/install-kafka.sh\", getInstallKafkaScript(dbfsDir, kafkaVersion))\n    writeFile(s\"/dbfs/$dbfsDir/configure-kafka.sh\", configureKafkaScript)\n  }\n  \n  /** Script that can be run on executors to install Kafka. */\n  private def getInstallKafkaScript(dbfsDir: String, kafkaVersion: String) = {\n    s\"\"\"#!/bin/bash\n       |set -e\n       |\n       |sudo chown ubuntu /home/ubuntu\n       |mkdir -p kafka && cd kafka\n       |if [ ! -r \"/dbfs/$dbfsDir/kafka-${kafkaVersion}.tgz\" ]; then\n       | wget -O /dbfs/$dbfsDir/kafka-${kafkaVersion}.tgz \"http://mirrors.advancedhosters.com/apache/kafka/${kafkaVersion}/kafka_2.11-${kafkaVersion}.tgz\"\n       |fi\n       |tar -xvzf /dbfs/$dbfsDir/kafka-${kafkaVersion}.tgz --strip 1 1> /dev/null 2>&1\n     \"\"\".stripMargin\n  }\n  \n  /** Default Kafka configuration file. */\n  private val configureKafkaScript = \n    \"\"\"#!/bin/bash\nset -e\n\ncd kafka\ncat > config/server.properties <<EOL\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id=$2\n\n############################# Zookeeper #############################\n\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\".\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect=$1:2181\n\n# Timeout in ms for connecting to zookeeper\nzookeeper.connection.timeout.ms=6000\n\n# Switch to enable topic deletion or not, default value is false\ndelete.topic.enable=true\n\nauto.create.topics.enable=false\n\n############################# Socket Server Settings #############################\n\n# The port the socket server listens on\nport=9092\n\n# Hostname the broker will bind to. If not set, the server will bind to all interfaces\nhost.name=$3\n\n# Hostname the broker will advertise to producers and consumers. If not set, it uses the\n# value for \"host.name\" if configured.  Otherwise, it will use the value returned from\n# java.net.InetAddress.getCanonicalHostName().\nadvertised.host.name=$3\n\n# The port to publish to ZooKeeper for clients to use. If this is not set,\n# it will publish the same port that the broker binds to.\n# advertised.port=9092\n\n# The number of threads handling network requests\nnum.network.threads=3\n \n# The number of threads doing disk I/O\nnum.io.threads=8\n\n# The send buffer (SO_SNDBUF) used by the socket server\nsocket.send.buffer.bytes=102400\n\n# The receive buffer (SO_RCVBUF) used by the socket server\nsocket.receive.buffer.bytes=102400\n\n# The maximum size of a request that the socket server will accept (protection against OOM)\nsocket.request.max.bytes=104857600\n\n# Use kafka broker receive time in message timestamps, instead of creation time\nlog.message.timestamp.type=LogAppendTime\n\n############################# Log Basics #############################\n\n# A comma seperated list of directories under which to store log files\nlog.dirs=/tmp/kafka-logs\n\n# The default number of log partitions per topic. More partitions allow greater\n# parallelism for consumption, but this will also result in more files across\n# the brokers.\nnum.partitions=8\n\n# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n# This value is recommended to be increased for installations with data dirs located in RAID array.\nnum.recovery.threads.per.data.dir=1\n\n\n############################# Log Retention Policy #############################\n\n# The following configurations control the disposal of log segments. The policy can\n# be set to delete segments after a period of time, or after a given size has accumulated.\n# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens\n# from the end of the log.\n\n# The minimum age of a log file to be eligible for deletion\nlog.retention.hours=168\n\n# The maximum size of a log segment file. When this size is reached a new log segment will be created.\nlog.segment.bytes=1073741824\n\n# The interval at which log segments are checked to see if they can be deleted according \n# to the retention policies\nlog.retention.check.interval.ms=300000\n\n# By default the log cleaner is disabled and the log retention policy will default to just delete segments after their retention expires.\n# If log.cleaner.enable=true is set the cleaner will be enabled and individual logs can then be marked for log compaction.\nlog.cleaner.enable=false\n\nEOL\n    \"\"\"\n}","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1497563500345,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2a6ca8c0-b730-4116-9c6b-a97dd2091f5c"},{"version":"CommandV1","origId":2044923475617473,"guid":"0db9f402-4995-4312-a6da-22f189f971da","subtype":"command","commandType":"auto","position":0.375,"command":"// Databricks notebook source\npackage com.databricks.spark\n\n// Imports from LocalKafka that I haven't touched, even though some are probably not used\nimport java.io.File\nimport java.nio.file.{Files, Path, Paths, StandardCopyOption}\nimport java.nio.file.attribute.PosixFilePermissions\nimport java.text.SimpleDateFormat\nimport java.util.{Date, Properties}\n\nimport java.io.File\n\nimport scala.collection.JavaConverters._\n\nimport org.apache.commons.io.FileUtils\nimport org.apache.commons.io.filefilter._\nimport java.io.{File, FileInputStream, FileOutputStream}\nimport java.util.jar.{JarEntry, JarOutputStream}\nimport java.util.jar.Attributes.Name\nimport java.util.jar.Manifest\n\nimport com.google.common.io.{ByteStreams, Files}\n\nimport org.apache.spark.sql._\n\nimport sys.process._\n\n/**\n * Class for managing Apache Flink on a Databricks cluster. Currently installs one taskmanager per EC2 instance.\n * It will also require an additional instance for the jobmanager if used in distributed mode.\n * Please do not use this code in production.\n *\n * @param spark SparkSession that will allow us to run Spark jobs to distribute ssh public keys to executors\n * @param numFlinkNodes Number of taskmanagers to launch. Must be smaller than or equal to the number of executors - 1,\n *                      unless used on Community Edition in local mode.\n * @param stopSparkOnFlinkNodes Whether to kill Spark executors on the instances we install Flink for better isolation\n * @param flinkVersion The version of Flink to install\n * @param numTaskSlots Number of task slots for each taskmanager. Set this to the number of CPUs available for the instance\n *                     types you use.\n */\nclass LocalFlink(\n    spark: SparkSession,\n    numFlinkNodes: Int = 1,\n    val stopSparkOnFlinkNodes: Boolean = false,\n    numTaskSlots: Int = 4,\n    flinkVersion: String = \"1.2.1\",\n    scalaVersion: String = \"2.11\") extends Serializable with SSHUtils {\n  import spark.implicits._\n  @transient val sc = spark.sparkContext\n  \n  import LocalFlink._\n  \n  require(numTaskSlots >= 1, \"Number of task slots per Flink taskmanager has to be greater than or equal to 1.\")\n  \n  private val workers: List[String] = {\n    val executors = sc.getExecutorMemoryStatus.keys.map(_.split(\":\").head).toSet\n    if (executors.size == 1) {\n      // just the driver\n      List(myIp)\n    } else {\n      (executors - myIp).toList\n    }\n  }\n  private val numExecutors = workers.length\n  def isJobManagerOnDriver = workers == List(myIp)\n  \n  require(numExecutors >= numFlinkNodes + 1 || (numExecutors == 1 && numExecutors == 1),\n    s\"\"\"You don't have enough executors to maintain $numFlinkNodes Flink task managers + the job manager. Please increase your cluster size,\n       |or decrease the amount of Flink nodes. Available executors: $numExecutors.\n     \"\"\".stripMargin)\n  if (stopSparkOnFlinkNodes) {\n    require(List(myIp) != workers, \"You can't stop Spark when running Flink only on the driver.\")\n  }\n  private lazy val flinkNodes = workers.take(numFlinkNodes + 1)\n  // assuming exactly one flink job manager.\n  private lazy val jobManager = flinkNodes.last\n  lazy val taskManagers = if (isJobManagerOnDriver) flinkNodes else flinkNodes.dropRight(1)\n\n  // jteoh: Flink workers (taskManagers) only require host name bc the master's startup script will ssh + start them.\n  // It does require that paths are the same on all machines, but that's already happening anyways.\n  lazy val taskManagersString = taskManagers.mkString(\",\") \n  private val dbfsDir = \"home/streaming/benchmark\"\n  \n  def init(): Unit = {\n    generateSshKeys()\n    writeInstallFile(dbfsDir, flinkVersion, scalaVersion)\n  }\n  \n  init()\n  \n  private def installFlink(): Unit = {\n    s\"bash /dbfs/$dbfsDir/install-flink.sh\".!!\n    flinkNodes.foreach { ip =>\n      ssh(ip, s\"bash /dbfs/$dbfsDir/install-flink.sh\")\n    }\n  }\n  \n  private def configureFlink(): Unit = {\n    if (!isJobManagerOnDriver) {\n      s\"bash /dbfs/$dbfsDir/configure-flink.sh $jobManager $taskManagersString $numTaskSlots\".!!\n      flinkNodes.foreach { host =>\n        ssh(host, s\"bash /dbfs/$dbfsDir/configure-flink.sh $jobManager $taskManagersString $numTaskSlots\")\n      }\n    } else {\n      // Akka actor system uses the `hostname` to bind the client which resolves to 127.0.1.1 instead of 127.0.0.1\n      val hostname = \"hostname\".!!\n      val hosts = s\"127.0.0.1 localhost\\n127.0.0.1 $hostname\"\n      writeFile(\"/etc/hosts\", hosts)\n      s\"bash /dbfs/$dbfsDir/configure-flink.sh $jobManager $taskManagersString $numTaskSlots\".!!\n    }\n  }\n  \n  /** Copy the dependencies to Flink's classpath. */\n  private def copyLibraries(): Unit = {\n    val copyLibScript = getClasspath().map { case (fileName, filePath) =>\n      s\"cp $filePath flink/lib/$fileName\"\n    }.mkString(\"\\n\")\n    writeFile(s\"/dbfs/$dbfsDir/copy-flink-libraries.sh\", copyLibScript)\n    val copyCommand = s\"bash /dbfs/$dbfsDir/copy-flink-libraries.sh\"\n    copyCommand.!!\n    if (!isJobManagerOnDriver) {\n      flinkNodes.foreach(ssh(_, copyCommand))\n    }\n  }\n  \n  private def startFlink(): Unit = {\n    if (isJobManagerOnDriver) {\n      \"flink/bin/start-local.sh\".!!\n    } else {\n      ssh(jobManager, \"flink/bin/jobmanager.sh start cluster\")\n      taskManagers.foreach { host =>\n        ssh(host, \"flink/bin/taskmanager.sh start\")\n      }\n    }\n  }\n  \n  /**\n   * Setup Flink in cluster\n   */\n  def setup(): Unit = {\n    println(s\"Job Manager: $jobManager\")\n    println(s\"Task Managers: $taskManagersString\")\n    val key = publicKey\n    sc.parallelize(0 until numExecutors, numExecutors).foreach { i =>\n      addAuthorizedPublicKey(key)\n    }\n\n    installFlink()\n    configureFlink()\n    copyLibraries()\n    startFlink()\n    \n    Thread.sleep(15 * 1000) // wait for Flink to come up - probably longer than actually required\n\n    if (stopSparkOnFlinkNodes) {\n      flinkNodes.foreach { ip =>\n        ssh(ip, s\"sudo monit stop spark-slave\")\n      }\n    }\n  }\n\n  def shutdown() {\n    // Normally you can use stop-cluster.sh, but this requires ssh access between nodes\n    // Alternatively, go to each node and call it yourself.\n    //https://ci.apache.org/projects/flink/flink-docs-release-1.2/setup/cluster_setup.html#adding-jobmanagertaskmanager-instances-to-a-cluster\n    //ssh(jobManager, s\"flink/bin/stop-cluster.sh\")\n    flinkNodes.foreach { host =>\n      println(s\"Stopping task manager on $host\")\n      ssh(host, \"flink/bin/taskmanager.sh stop\")\n    }\n    println(s\"Stopping job manager on $jobManager\")\n    ssh(jobManager, s\"flink/bin/jobmanager.sh stop cluster\")\n  }\n  \n  /** Get all the dependencies required for running the benchmark. */\n  private def getClasspath(): Seq[(String, String)] = {\n    case class MavenCoordinate(orgId: String, artifactId: String, version: String) {\n      def name: String = s\"${artifactId}-${version}.jar\"\n      def path: String = s\"/dbfs/FileStore/jars/maven/${orgId.replace(\".\", \"/\")}/${artifactId}-${version}.jar\"\n    }\n    Seq(\n      MavenCoordinate(\"io.spray\", \"spray-json_2.11\", \"1.3.3\"),\n      MavenCoordinate(\"org.apache.kafka\", \"kafka-streams\", \"0.10.2.1\"),\n      MavenCoordinate(\"org.apache.kafka\", \"kafka-clients\", \"0.10.2.1\"),\n      MavenCoordinate(\"org.apache.kafka\", \"connect-api\", \"0.10.2.1\"),\n      MavenCoordinate(\"org.apache.kafka\", \"connect-json\", \"0.10.2.1\"),\n      MavenCoordinate(\"org.apache.flink\", \"flink-streaming-scala_2.11\", flinkVersion),\n      MavenCoordinate(\"org.apache.flink\", \"flink-streaming-java_2.11\", flinkVersion),\n      MavenCoordinate(\"org.apache.flink\", \"flink-scala_2.11\", flinkVersion),\n      MavenCoordinate(\"org.apache.flink\", \"flink-connector-kafka-0.10_2.11\", flinkVersion),\n      MavenCoordinate(\"org.apache.flink\", \"flink-connector-kafka-0.9_2.11\", flinkVersion),\n      MavenCoordinate(\"org.apache.flink\", \"flink-connector-kafka-base_2.11\", flinkVersion),\n      MavenCoordinate(\"org.rocksdb\", \"rocksdbjni\", \"5.0.1\")\n    ).map(coord => (coord.name, coord.path))\n  } \n  \n  /** Run a jar as a Flink job. */\n  def runJob(className: String, args: String*): Unit = {\n    val jar = packJar(spark)\n    (Seq(\"flink/bin/flink\", \"run\", \"-c\", className, jar.toString) ++ args).!!\n  }\n}\n\n// Same deal - unless otherwise noted, assume same as LocalKafka\nobject LocalFlink extends Serializable {\n  var cluster: LocalFlink = null\n\n  def setup(\n      spark: SparkSession,\n      numFlinkNodes: Int = 1,\n      stopSparkOnFlinkNodes: Boolean = false,\n      numTaskSlots: Int = 4,\n      flinkVersion: String = \"1.2.1\",\n      scalaVersion: String = \"2.11\"): LocalFlink = {\n    if(cluster == null) {\n      cluster = new LocalFlink(spark, numFlinkNodes, stopSparkOnFlinkNodes, numTaskSlots, flinkVersion, scalaVersion)\n      cluster.setup()\n    }\n    cluster\n  }\n  \n  def shutdown() {\n    if (cluster != null) {\n      cluster.shutdown()\n      cluster = null\n    }\n  }\n  private def myIp = {\n    \"hostname\".!!.split(\"_\").takeRight(4).map(_.trim).mkString(\".\")\n  }\n  \n  def ssh(host: String, command: String): Unit = {\n    if (cluster != null) {\n      cluster.ssh(host, command)\n    } else {\n      println(\"No cluster set up!\")\n    }\n  }\n  \n  private def writeFile(path: String, contents: String, append: Boolean = false): Unit = {\n    val fw = new java.io.FileWriter(path, append)\n    fw.write(contents)\n    fw.close()\n  }\n  \n  private def writeInstallFile(dbfsDir: String, flinkVersion: String, scalaVersion: String): Unit = {\n    Seq(\"mkdir\", \"-p\", s\"/dbfs/$dbfsDir\").!!\n    writeFile(s\"/dbfs/$dbfsDir/install-flink.sh\", getInstallFlinkScript(dbfsDir, flinkVersion, scalaVersion))\n    writeFile(s\"/dbfs/$dbfsDir/configure-flink.sh\", configureFlinkScript)\n  }\n  \n  /** Script to install Flink on an executor. */\n  private def getInstallFlinkScript(dbfsDir: String, flinkVersion: String, scalaVersion: String = \"2.11\") = {\n    s\"\"\"#!/bin/bash\n       |set -e\n       |\n       |sudo chown ubuntu /home/ubuntu\n       |mkdir -p flink && cd flink\n       |if [ ! -r \"/dbfs/$dbfsDir/flink-${flinkVersion}.tgz\" ]; then\n       | wget -O /dbfs/$dbfsDir/flink-${flinkVersion}.tgz \"http://mirrors.advancedhosters.com/apache/flink/flink-${flinkVersion}/flink-${flinkVersion}-bin-hadoop27-scala_${scalaVersion}.tgz\"\n       |fi\n       |tar -xvzf /dbfs/$dbfsDir/flink-${flinkVersion}.tgz --strip 1 1> /dev/null 2>&1\n     \"\"\".stripMargin\n  }\n\n  /**\n   * Script to configure Flink on an executor.\n   * First argument = jobmanager (replaced jobmanager.rpc.address via sed, orig file as .bak)\n   * Second argument = comma-separated list of taskmanagers, which gets written to 'slaves' file\n   * Third argument = number of task slots for reach taskmanager\n   * We set the heap sizes a bit higher than the default configurations here, but we don't have a memory intensive workload\n   * therefore don't need to fine tune that.\n   */\n  private val configureFlinkScript = \n    \"\"\"#!/bin/bash\nset -e\n\ncd flink/conf\nsed -i.bak -e \"s/jobmanager.rpc.address:.*/jobmanager.rpc.address: $1/\" flink-conf.yaml\nsed -i.bak -e \"s/taskmanager.numberOfTaskSlots: 1/taskmanager.numberOfTaskSlots: $3/\" flink-conf.yaml\nsed -i.bak -e \"s/jobmanager.heap.mb:.*/jobmanager.heap.mb: 2048/\" flink-conf.yaml\nsed -i.bak -e \"s/taskmanager.heap.mb:.*/taskmanager.heap.mb: 3096/\" flink-conf.yaml\n\ntr ',' '\\n' <<<\"$2\" >slaves\necho Flink slaves:\ncat slaves\necho\n\n    \"\"\"\n  \n  /** Packs a jar of compiled `package` cells so that we can submit a Databricks cell as a Flink job. */\n  private def packJar(spark: SparkSession): File = {\n    val compiledClassesDir = spark.conf.get(\"spark.repl.class.outputDir\")\n    \n    val filter = new IOFileFilter {\n      override def accept(file: File): Boolean = {\n        accept(file.getParentFile, file.getName)\n      }\n\n      override def accept(dir: File, name: String): Boolean = {\n        val subDir = Seq(\"com\", \"databricks\", \"benchmark\").mkString(File.separator)\n        dir.toString.contains(subDir) && name.endsWith(\".class\")\n      }\n    }\n\n    val filesToPack = FileUtils.listFiles(new File(compiledClassesDir), filter, TrueFileFilter.INSTANCE).asScala.map { file =>\n      file.toString.replace(compiledClassesDir + File.separator, \"\") -> file\n    }\n\n    val jarFile = new File(\"/tmp\", \"benchmark.jar\")\n    val jarFileStream = new FileOutputStream(jarFile)\n    val jarStream = new JarOutputStream(jarFileStream, new Manifest())\n\n    for (file <- filesToPack) {\n      val jarEntry = new JarEntry(file._1)\n      jarStream.putNextEntry(jarEntry)\n\n      val in = new FileInputStream(file._2)\n      ByteStreams.copy(in, jarStream)\n      in.close()\n    }\n    jarStream.close()\n    jarFileStream.close()\n\n    jarFile\n  }\n}","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1497563507562,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"982cd07a-52f2-479d-a42c-dfdc2a23ced4"},{"version":"CommandV1","origId":2044923475617474,"guid":"3a353c56-d765-4912-83c2-ae306adac979","subtype":"command","commandType":"auto","position":0.5,"command":"package com.databricks.benchmark.utils\n\nimport scala.reflect._\n\nimport com.fasterxml.jackson.databind.ObjectMapper\nimport com.fasterxml.jackson.module.scala.DefaultScalaModule\nimport com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper\n\n/** \n * Utility functions for Json munging using Jackson. We use spray for Flink as there is a Jackson version incompatiblity between\n * Spark's and Flink's jackson versions. Our tests did not show any significant performance degradation using one over the other.\n */\nobject Json {\n  private val parser: ObjectMapper = new ObjectMapper() with ScalaObjectMapper\n  parser.registerModule(DefaultScalaModule)\n  \n  def parse[T: ClassTag](json: String): T = parser.readValue(json, classTag[T].runtimeClass).asInstanceOf[T]\n  \n  def parseBytes[T: ClassTag](json: Array[Byte]): T = parser.readValue(json, classTag[T].runtimeClass).asInstanceOf[T]\n  \n  def getString[T](obj: T): String = parser.writeValueAsString(obj)\n  \n  def getBytes[T](obj: T): Array[Byte] = parser.writeValueAsBytes(obj)\n}","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1496441711822,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d0ba08c0-c9bb-4281-984a-8364a891cdc2"},{"version":"CommandV1","origId":2044923475617475,"guid":"5f62d501-317e-4f78-957d-9ec03852a5c6","subtype":"command","commandType":"auto","position":0.75,"command":"package com.databricks.benchmark.yahoo\n\n/** Classes we use in the Yahoo Benchmark. */\n\n/** Input data */\ncase class Event(\n  user_id: String,         // UUID\n  page_id: String,         // UUID\n  ad_id: String,           // UUID\n  ad_type: String,         // in {banner, modal, sponsored-search, mail, mobile}\n  event_type: String,      // in {view, click, purchase}\n  event_time: java.sql.Timestamp,\n  ip_address: String) {\n  def this() = this(null, null, null, null, null, new java.sql.Timestamp(0L), null)\n}\n\n/** Output data */\ncase class Output(\n  time_window: java.sql.Timestamp,\n  campaign_id: String,\n  count: Long,\n  lastUpdate: java.sql.Timestamp) {\n  def this() = this(new java.sql.Timestamp(0L), null, 0L, new java.sql.Timestamp(0L))\n}\n\n/** Event data after a projection */\ncase class ProjectedEvent(\n  ad_id: String,           // UUID\n  event_time: java.sql.Timestamp) {\n  def this() = this(null, new java.sql.Timestamp(0L))\n}\n\n/** Used for forming a map of ad to campaigns. */\ncase class CampaignAd(ad_id: String, campaign_id: String) {\n  def this() = this(null, null)\n}\n\n/** Static variables used through out the benchmark. */\nobject Variables {\n  val CAMPAIGNS_TOPIC = \"campaigns\"\n  val EVENTS_TOPIC = \"events\"\n  val OUTPUT_TOPIC = \"output\"\n  \n  val AD_TYPES = Seq(\"banner\", \"modal\", \"sponsored-search\", \"mail\", \"mobile\")\n  val EVENT_TYPES = Seq(\"view\", \"click\", \"purchase\")\n}","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1496441714347,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d02e8eb0-6c1d-4263-b55b-53e42d7bd4d5"},{"version":"CommandV1","origId":2044923475617477,"guid":"fab1383b-cda5-46b7-bcdd-bc440121d537","subtype":"command","commandType":"auto","position":0.9375,"command":"package com.databricks.benchmark.yahoo\n\nimport java.util.UUID\n\nimport org.apache.spark.sql.{DataFrame, SparkSession}\nimport org.apache.spark.sql.functions._\n\ntrait YahooBenchmarkRunner {\n  import com.databricks.spark.LocalKafka\n  \n  val spark: SparkSession\n  \n  import spark.implicits._\n  \n  /** \n   * Start the reader for the system. This should be called before `generateData` so that reader initialization doesn't cause any\n   * latency penalties.\n   */\n  def start(): Unit\n  \n  /**\n   * Stop the runner.\n   */\n  def stop(): Unit = {\n    spark.streams.active.foreach(_.stop())\n  }\n  \n  /** Runner specific parameters that should be saved among benchmark parameters. */\n  val params: Map[String, Any] = Map.empty\n  \n  /**\n   * Generate data for the benchmark. The Spark and Flink runners generate the data themselves, therefore for Kafka,\n   * we need to push data into it ourselves.\n   */\n  def generateData(\n      campaigns: Array[CampaignAd],\n      tuplesPerSecond: Long,\n      recordGenParallelism: Int,\n      rampUpTimeSeconds: Int): Unit = {\n    campaigns.toSeq.toDS().select(to_json(struct(\"*\")) as 'value, 'ad_id as 'key).write\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", LocalKafka.cluster.kafkaNodesString)\n      .option(\"topic\", Variables.CAMPAIGNS_TOPIC)\n      .save()\n\n    YahooBenchmarkRunner.generateStream(spark, campaigns, tuplesPerSecond, recordGenParallelism, rampUpTimeSeconds)\n      .select(to_json(struct(\"*\")) as 'value, 'ad_id as 'key)\n      .writeStream\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", LocalKafka.cluster.kafkaNodesString)\n      .option(\"topic\", Variables.EVENTS_TOPIC)\n      .option(\"checkpointLocation\", s\"/tmp/${UUID.randomUUID()}\")\n      .start()\n  }\n  \n  /** Calculate the throughput of this runner. */\n  def getThroughput(): DataFrame\n  \n  /** Calculate the latency of this runner. */\n  def getLatency(): DataFrame\n}\n\nobject YahooBenchmarkRunner {\n  /**\n   * Default event generator that creates a streaming DataFrame of `Events` that can then be pushed to Kafka\n   * or consumed directly by Spark.\n   */\n  def generateStream(\n      spark: SparkSession,\n      campaigns: Array[CampaignAd],\n      tuplesPerSecond: Long,\n      numPartitions: Int,\n      rampUpTimeSeconds: Int): DataFrame = {\n    import spark.implicits._\n    val adTypeLength = Variables.AD_TYPES.length\n    val eventTypeLength = Variables.EVENT_TYPES.length\n    val campaignLength = campaigns.length\n    val getAdType = udf((i: Int) => Variables.AD_TYPES(i % adTypeLength))\n    val getEventType = udf((i: Int) => Variables.EVENT_TYPES(i % eventTypeLength))\n    val getCampaign = udf((i: Int) => campaigns(i % campaignLength).ad_id)\n\n    val uuid = UUID.randomUUID().toString\n    \n    spark.readStream\n      .format(\"rate\")\n      .option(\"rowsPerSecond\", tuplesPerSecond.toString)\n      .option(\"numPartitions\", numPartitions.toString)\n      .option(\"rampUpTime\", rampUpTimeSeconds.toString + \"s\")\n      .load()\n      .select(\n        lit(uuid) as 'user_id,\n        lit(uuid) as 'page_id,\n        getCampaign('value % 100000) as 'ad_id,\n        getAdType('value % 100000) as 'ad_type,\n        getEventType('value % 100000) as 'event_type,\n        current_timestamp() as 'event_time,\n        lit(\"255.255.255.255\") as 'ip_address)\n  }\n}","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1497036075966,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"55be3962-38ab-48b0-88fe-e41cd7a44987"},{"version":"CommandV1","origId":2044923475617478,"guid":"10a39f5c-0cb3-41c8-a196-fe17b249dc14","subtype":"command","commandType":"auto","position":0.96875,"command":"import scala.collection.JavaConverters._\n\nimport org.apache.spark.sql.SparkSession\n\nimport com.databricks.benchmark.yahoo.YahooBenchmarkRunner\n\ntrait Benchmark[Runner <: YahooBenchmarkRunner] {\n  var runner: Runner = _\n  val tuplesPerSecond: Long\n  val recordGenParallelism: Int\n  val rampUpTimeSeconds: Int\n  \n  protected val benchmarkParams: Map[String, Any] = Map.empty\n  \n  private lazy val allParams = benchmarkParams ++ runner.params ++ Map(\n    \"tuplesPerSecond\" -> tuplesPerSecond,\n    \"recordGenParallelism\" -> recordGenParallelism,\n    \"rampUpTimeSeconds\" -> rampUpTimeSeconds)\n  \n  private var readerThread: Thread = _\n  protected val dontInterrupt = false\n  \n  var stoppedReader = false\n  \n  protected val readerWaitTimeMs: Long = 300000 // 5 minutes\n\n  protected def init(): Unit = {\n    if (readerThread != null) {\n      try {\n        if (!dontInterrupt) {\n          readerThread.interrupt()\n        }\n        readerThread.join()\n      } catch {\n        case _: InterruptedException =>\n      }\n    }\n    stoppedReader = false\n  }\n\n  protected def startReader(): Unit\n  \n  private def stopReader0(): Unit = {\n    if (!stoppedReader) {\n      stopReader()\n      stoppedReader = true\n    }\n  }\n  \n  protected def stopReader(): Unit\n\n  protected def produceRecords(): Unit\n    \n  private def timeIt(stage: String)(f: => Unit): Unit = {\n    val start = System.nanoTime\n    f\n    println(s\"$stage took ${(System.nanoTime - start) / 1e9} seconds\")\n  }\n  \n  final def run(runner: Runner, outputPath: String, numRuns: Int = 10): Unit = {\n    this.runner = runner\n    dbutils.fs.put(outputPath.stripSuffix(\"/\") + \"/_parameters\", \n      allParams.map { case (key, value) => s\"$key: $value\" }.mkString(\"\\n\"), overwrite = true)\n    for (i <- 1 to numRuns) {\n      timeIt(s\"run $i\") {\n        runOnce(i, outputPath)\n      }\n      System.gc()\n    }\n  }\n  \n  protected def saveResults(outputPath: String, trial: Int): Unit\n    \n  private def runOnce(trial: Int, outputPath: String): Unit = {\n    timeIt(\"initialization\") {\n      init()\n    }\n    timeIt(\"starting reader\") {\n      readerThread = new Thread(\"KafkaReader\") {\n        override def run(): Unit = {\n          try {\n            startReader()\n          } catch {\n            case t: Throwable =>\n              println(t)\n              println(t.getStackTrace.mkString(\"\\n\"))\n              println(t.getCause)\n              throw t\n          }\n          println(\"reader exiting\")\n        }\n      }\n      readerThread.setDaemon(true)\n      readerThread.start()\n      Thread.sleep(5000)\n    }\n    try {\n      assert(readerThread.isAlive, \"Reader thread died for a reason\")\n      val producerThread = new Thread(\"DataProducer\") {\n        override def run(): Unit = {\n          try {\n            timeIt(\"producing records\") {\n              produceRecords()\n            }\n          } catch {\n            case t: Throwable =>\n              println(t)\n              println(t.getStackTrace.mkString(\"\\n\"))\n              println(t.getCause)\n              throw t\n          }\n        }\n      }\n      producerThread.setDaemon(true)\n      producerThread.start()\n        \n      val start = System.currentTimeMillis\n      while ((System.currentTimeMillis - start) < readerWaitTimeMs && readerThread.isAlive) {\n        Thread.sleep(1000)\n      }\n      stopReader()\n      saveResults(outputPath, trial)\n    } finally {\n      stopReader()\n    }\n  }    \n}","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1496436911439,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"1b0cbf94-0413-4f67-b76b-51702adad1a4"},{"version":"CommandV1","origId":2044923475617479,"guid":"b156195e-dc3b-4699-8e4d-1f929db9c417","subtype":"command","commandType":"auto","position":0.984375,"command":"import java.util.UUID\n\nimport org.apache.spark.sql.{DataFrame, Encoders}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\n\nimport com.databricks.spark.LocalKafka\n\nimport com.databricks.benchmark.yahoo._\n\n/**\n * Benchmark for measuring throughput and latency. Details available at: \n * [[https://yahooeng.tumblr.com/post/135321837876/benchmarking-streaming-computation-engines-at]].\n */\nclass YahooBenchmark(\n    kafkaCluster: LocalKafka,\n    override val tuplesPerSecond: Long,\n    override val recordGenParallelism: Int,\n    override val rampUpTimeSeconds: Int,\n    kafkaEventsTopicPartitions: Int = 1,\n    kafkaOutputTopicPartitions: Int = 1,\n    numCampaigns: Int = 100,\n    override val readerWaitTimeMs: Long = 300000) extends Benchmark[YahooBenchmarkRunner] {\n  \n  override val benchmarkParams: Map[String, Any] = Map(\n    \"numCampaigns\" -> numCampaigns,\n    \"kafkaEventsTopicPartitions\" -> kafkaEventsTopicPartitions,\n    \"kafkaOutputTopicPartitions\" -> kafkaOutputTopicPartitions)\n    \n  override protected def init(): Unit = {\n    super.init()\n    kafkaCluster.deleteTopicIfExists(Variables.OUTPUT_TOPIC)\n    kafkaCluster.createTopic(Variables.OUTPUT_TOPIC, partitions = kafkaOutputTopicPartitions, replFactor = 1)\n    kafkaCluster.deleteTopicIfExists(Variables.CAMPAIGNS_TOPIC)\n    kafkaCluster.createTopic(Variables.CAMPAIGNS_TOPIC, partitions = kafkaEventsTopicPartitions, replFactor = 1)\n    kafkaCluster.deleteTopicIfExists(Variables.EVENTS_TOPIC)\n    kafkaCluster.createTopic(Variables.EVENTS_TOPIC, partitions = kafkaEventsTopicPartitions, replFactor = 1)\n  }\n  \n  lazy val campaigns = spark.range(1, numCampaigns).flatMap { e =>\n    val campaign = UUID.randomUUID().toString\n    Seq.tabulate(10)(_ => CampaignAd(UUID.randomUUID().toString, campaign))\n  }.collect()\n  \n  override protected def produceRecords(): Unit = {\n    runner.generateData(campaigns, tuplesPerSecond, recordGenParallelism, rampUpTimeSeconds)\n  }\n  \n  override protected def startReader(): Unit = {\n    runner.start()\n  }\n\n  override protected def stopReader(): Unit = {\n    runner.stop()\n  }\n  \n  override protected def saveResults(outputPath: String, trial: Int): Unit = {\n    val throughput = runner.getThroughput()\n    val latency = runner.getLatency()\n    throughput.crossJoin(latency).coalesce(1).write.mode(\"overwrite\").json(outputPath.stripSuffix(\"/\") + s\"/trial=$trial\")\n  }\n}\n\nobject YahooBenchmark {\n  val outputSchema = new StructType()\n    .add(\"time_window\", LongType)\n    .add(\"campaign_id\", StringType)\n    .add(\"count\", LongType)\n  \n  def getBenchmarkResults(outputPath: String): DataFrame = {\n    val df = spark.read.json(outputPath)\n      .select(\n        'trial,\n        'start,\n        'end,\n        'totalDurationMillis,\n        'totalInput as 'recordsProcessed,\n        'throughput,\n        'latency_min,\n        'latency_95,\n        'latency_99,\n        'latency_max,\n        'latency_avg)\n    display(df.orderBy('trial))\n    df\n  }\n}","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1496436926089,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"7b6af53b-f8a6-4957-b99f-6e2529f3a3cc"}],"dashboards":[],"guid":"33a966bc-0477-4af2-9e33-2a8b34cd4a8b","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":0,"guid":"756717cf-fd4e-4cc4-b00e-3f7ab18b778f","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{}},"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{}};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>