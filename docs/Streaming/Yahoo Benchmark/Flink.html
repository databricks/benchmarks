<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Streaming / Yahoo Benchmark / Flink - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta name="robots" content="nofollow">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableNotebookNotifications":true,"enableSshKeyUI":false,"defaultInteractivePricePerDBU":0.4,"enableClusterMetricsUI":true,"useReactTableCreateView":false,"enableOnDemandClusterType":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","enableJobsPrefetching":true,"workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/index.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableClearStateFeature":true,"enableJobsAclsV2InUI":false,"dbcForumURL":"http://forums.databricks.com/","enableProtoClusterInfoDeltaPublisher":true,"enableAttachExistingCluster":true,"resetJobListOnConnect":true,"serverlessDefaultSparkVersion":"latest-stable-scala2.11","maxCustomTags":45,"serverlessDefaultMaxWorkers":20,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"support_ssh":false,"spark_heap_memory":4800,"instance_type_id":"r3.2xlarge","spark_core_oversubscription_factor":8.0,"node_type_id":"dev-tier-node","description":"Community Optimized","support_cluster_tags":false,"container_memory_mb":6000,"node_instance_type":{"instance_type_id":"r3.2xlarge","provider":"AWS","local_disk_size_gb":160,"compute_units":26.0,"number_of_ips":14,"local_disks":1,"reserved_compute_units":3.64,"gpus":0,"memory_mb":62464,"num_cores":8,"local_disk_type":"AHCI","max_attachable_disks":0,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":6144,"is_hidden":false,"category":"Community Edition","num_cores":0.88,"support_port_forwarding":false,"support_ebs_volumes":false,"is_deprecated":false}],"default_node_type_id":"dev-tier-node"},"sqlAclsDisabledMap":{"spark.databricks.acl.enabled":"false","spark.databricks.acl.sqlOnly":"false"},"enableDatabaseSupportClusterChoice":true,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":999999,"serverlessClusterProductName":"Serverless Pool","showS3TableImportOption":true,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"timerUpdateQueueLength":100,"sqlAclsEnabledMap":{"spark.databricks.acl.enabled":"true","spark.databricks.acl.sqlOnly":"true"},"enableLargeResultDownload":true,"maxElasticDiskCapacityGB":5000,"serverlessDefaultMinWorkers":2,"zoneInfos":[{"id":"us-west-2c","isDefault":true},{"id":"us-west-2b","isDefault":false},{"id":"us-west-2a","isDefault":false}],"enableCustomSpotPricingUIByTier":false,"serverlessClustersEnabled":false,"enableFindAndReplace":true,"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":true,"enableBitbucketCloud":true,"enableMaxConcurrentRuns":true,"enableJobAclsConfig":false,"enableFullTextSearch":false,"enableElasticSparkUI":false,"enableNewClustersCreate":true,"clusters":true,"allowRunOnPendingClusters":true,"useAutoscalingByDefault":false,"enableAzureToolbar":false,"fileStoreBase":"FileStore","enableEmailInAzure":false,"enableRLibraries":true,"enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":true,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":false,"checkBeforeAddingAadUser":false,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"perClusterAutoterminationEnabled":false,"enableNotebookCommandNumbers":true,"sparkVersions":[{"key":"1.6.3-db2-hadoop2-scala2.10","displayName":"Spark 1.6.3-db2 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-aba860a0ffce4f3471fb14aefdcb1d768ac66a53a5ad884c48745ef98aeb9d67","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"3.3.x-gpu-scala2.11","displayName":"3.3 (includes Apache Spark 2.2.0, GPU, Scala 2.11)","packageLabel":"spark-image-4ccf711c90fd8d633b5629f18c840ae0a337b85583bb20438c73de5a94f19099","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db5-scala2.11","displayName":"Spark 2.1.1-db5 (Scala 2.11)","packageLabel":"spark-image-08d9fc1551087e0876236f19640c4a83116b1649f15137427d21c9056656e80e","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.3.x-scala2.10","displayName":"3.3 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-88f10af7324d00498d3cb8f6d808d32d5c60474845a5c264096b409ade4f25fb","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1, deprecated)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.2.x-scala2.11","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-67ab3a06d1e83d5b60df7063245eb419a2e9fe329aeeb7e7d9713332c669bb17","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.1-db6-scala2.10","displayName":"Spark 2.1.1-db6 (Scala 2.10)","packageLabel":"spark-image-177f3f02a6a3432d30068332dc857b9161345bdd2ee8a2d2de05bb05cb4b0f4c","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.11","displayName":"Spark 2.1.0-db2 (Scala 2.11)","packageLabel":"spark-image-267c4490a3ab8a39acdbbd9f1d36f6decdecebf013e30dd677faff50f1d9cf8b","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.x-gpu-scala2.11","displayName":"Spark 2.1 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-d613235f93e0f29838beb2079a958c02a192ed67a502192bc67a8a5f2fb37f35","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"latest-stable-gpu-scala2.11","displayName":"Latest stable (3.3, GPU, Scala 2.11)","packageLabel":"spark-image-4ccf711c90fd8d633b5629f18c840ae0a337b85583bb20438c73de5a94f19099","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db3-scala2.10","displayName":"Spark 2.0.2-db3 (Scala 2.10)","packageLabel":"spark-image-584091dedb690de20e8cf22d9e02fdcce1281edda99eedb441a418d50e28088f","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.2.x-scala2.10","displayName":"3.2 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-3ef6d6cc156adc3024eaff9e47af444e30a5cc9e61d09c34e8ae9e1b0a4e4960","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"latest-experimental-scala2.10","displayName":"Latest experimental (3.3 snapshot, Scala 2.10)","packageLabel":"spark-image-09756b249790a7c8ef49f9bfe143f5a659e69a3515b703d654db5e92a08a2883","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0-db1 (Scala 2.11)","packageLabel":"spark-image-e8ad5b72cf0f899dcf2b4720c1f572ab0e87a311d6113b943b4e1d4a7edb77eb","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.1-db4-scala2.11","displayName":"Spark 2.1.1-db4 (Scala 2.11)","packageLabel":"spark-image-52bca0ca866e3f4243d3820a783abf3b9b3b553edf234abef14b892657ceaca9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"latest-rc-scala2.11","displayName":"Latest RC (3.3, Scala 2.11)","packageLabel":"spark-image-8a04b8efdccbf114de89078cbdea89452111fbdb6f89fbed9f128e5b778d6e95","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"latest-stable-scala2.11","displayName":"Latest stable (3.3, Scala 2.11)","packageLabel":"spark-image-d9361b34430b88997c4e8d9bb3e2003e77cfbd7af75948968fdb30a40338cd43","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.0-db2-scala2.10","displayName":"Spark 2.1.0-db2 (Scala 2.10)","packageLabel":"spark-image-a2ca4f6b58c95f78dca91b1340305ab3fe32673bd894da2fa8e1dc8a9f8d0478","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db4-scala2.11","displayName":"Spark 2.0.2-db4 (Scala 2.11)","packageLabel":"spark-image-7dbc7583e8271765b8a1508cb9e832768e35489bbde2c4c790bc6766aee2fd7f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-968b89f1d0ec32e1ee4dacd04838cae25ef44370a441224177a37980d539d83a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-eaa8d9b990015a14e032fb2e2e15be0b8d5af9627cd01d855df728b67969d5d9","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.3-db2-hadoop1-scala2.10","displayName":"Spark 1.6.3-db2 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-14112ea0645bea94333a571a150819ce85573cf5541167d905b7e6588645cf3b","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-36d48f22cca7a907538e07df71847dd22aaf84a852c2eeea2dcefe24c681602f","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-8e1c50d626a52eac5a6c8129e09ae206ba9890f4523775f77af4ad6d99a64c44","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-859e88079f97f58d50e25163b39a1943d1eeac0b6939c5a65faba986477e311a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db4-scala2.10","displayName":"Spark 2.1.1-db4 (Scala 2.10)","packageLabel":"spark-image-c7c0224de396cd1563addc1ae4bca6ba823780b6babe6c3729ddf73008f29ba4","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"latest-rc-scala2.10","displayName":"Latest RC (3.3, Scala 2.10)","packageLabel":"spark-image-09756b249790a7c8ef49f9bfe143f5a659e69a3515b703d654db5e92a08a2883","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"latest-stable-scala2.10","displayName":"Latest stable (3.3, Scala 2.10)","packageLabel":"spark-image-88f10af7324d00498d3cb8f6d808d32d5c60474845a5c264096b409ade4f25fb","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-c2d623f03dd44097493c01aa54a941fc31978ebe6d759b36c75b716b2ff6ab9c","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db4-scala2.10","displayName":"Spark 2.0.2-db4 (Scala 2.10)","packageLabel":"spark-image-859e88079f97f58d50e25163b39a1943d1eeac0b6939c5a65faba986477e311a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db5-scala2.10","displayName":"Spark 2.1.1-db5 (Scala 2.10)","packageLabel":"spark-image-74133df2c13950431298d1cab3e865c191d83ac33648a8590495c52fc644c654","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1, deprecated)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"latest-experimental-gpu-scala2.11","displayName":"Latest experimental (3.3 snapshot, GPU, Scala 2.11)","packageLabel":"spark-image-5feb7cda877fd9b391835ad1b430973dcb675bd52e8acc83bd3b663e3960fc4a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.2.x-scala2.10","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-d549f2d4a523994ecdf37e531b51d5ec7d8be51534bb0ca5322eaad28ba8f557","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.0.x-scala2.11","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-67ab3a06d1e83d5b60df7063245eb419a2e9fe329aeeb7e7d9713332c669bb17","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-7dbc7583e8271765b8a1508cb9e832768e35489bbde2c4c790bc6766aee2fd7f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-177f3f02a6a3432d30068332dc857b9161345bdd2ee8a2d2de05bb05cb4b0f4c","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"3.1.x-scala2.11","displayName":"3.1 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-0e50da7a73d3bc76fcca8ac8a67c1e036c5a30deb5663adf6d0f332cc8ec2c90","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.0-db3-scala2.10","displayName":"Spark 2.1.0-db3 (Scala 2.10)","packageLabel":"spark-image-25a17d070af155f10c4232dcc6248e36a2eb48c24f8d4fc00f34041b86bd1626","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-4fa852ba378e97815083b96c9cada7b962a513ec23554a5fc849f7f1dd8c065a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"3.1.x-scala2.10","displayName":"3.1 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-0c03d5f78139022c37032b5f3ffac5b5a4445d9cab8733e333f16a67a47262f9","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.3.x-scala2.11","displayName":"3.3 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-d9361b34430b88997c4e8d9bb3e2003e77cfbd7af75948968fdb30a40338cd43","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1, deprecated)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db3-scala2.11","displayName":"Spark 2.0.2-db3 (Scala 2.11)","packageLabel":"spark-image-7fd7aaa89d55692e429115ae7eac3b1a1dc4de705d50510995f34306b39c2397","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.1-db6-scala2.11","displayName":"Spark 2.1.1-db6 (Scala 2.11)","packageLabel":"spark-image-fdad9ef557700d7a8b6bde86feccbcc3c71d1acdc838b0fd299bd19956b1076e","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-d50af1032799546b8ccbeeb76889a20c819ebc2a0e68ea20920cb30d3895d3ae","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-654bdd6e9bad70079491987d853b4b7abf3b736fff099701501acaabe0e75c41","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-a659f3909d51b38d297b20532fc807ecf708cfb7440ce9b090c406ab0c1e4b7e","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"latest-experimental-scala2.11","displayName":"Latest experimental (3.3 snapshot, Scala 2.11)","packageLabel":"spark-image-8a04b8efdccbf114de89078cbdea89452111fbdb6f89fbed9f128e5b778d6e95","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.2.x-scala2.11","displayName":"3.2 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-aca37a44b05c9c0177ca8a639ad186ebace4aac1a7bdcf73829945ab47414f41","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.x-scala2.11","displayName":"Spark 2.1 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-fdad9ef557700d7a8b6bde86feccbcc3c71d1acdc838b0fd299bd19956b1076e","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0-db1 (Scala 2.10)","packageLabel":"spark-image-f0ab82a5deb7908e0d159e9af066ba05fb56e1edb35bdad41b7ad2fd62a9b546","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"3.0.x-scala2.10","displayName":"3.0 (includes Apache Spark 2.2.0, Scala 2.10)","packageLabel":"spark-image-d549f2d4a523994ecdf37e531b51d5ec7d8be51534bb0ca5322eaad28ba8f557","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.0-db3-scala2.11","displayName":"Spark 2.1.0-db3 (Scala 2.11)","packageLabel":"spark-image-ccbc6b73f158e2001fc1fb8c827bfdde425d8bd6d65cb7b3269784c28bb72c16","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"latest-rc-gpu-scala2.11","displayName":"Latest RC (3.3 GPU, Scala 2.11)","packageLabel":"spark-image-5feb7cda877fd9b391835ad1b430973dcb675bd52e8acc83bd3b663e3960fc4a","upgradable":true,"deprecated":false,"customerVisible":false}],"enablePresentationMode":false,"enableClearStateAndRunAll":true,"enableRestrictedClusterCreation":true,"enableFeedback":true,"enableClusterAutoScaling":false,"enableUserVisibleDefaultTags":true,"defaultNumWorkers":0,"serverContinuationTimeoutMillis":10000,"jobsUnreachableThresholdMillis":60000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":false,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","showDbuPricing":true,"databricksDocsBaseHostname":"docs.databricks.com","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"i3.4xlarge":4,"class-node":1,"m4.2xlarge":1.5,"r4.xlarge":1,"m4.4xlarge":3,"r4.16xlarge":16,"Standard_DS11":0.5,"p2.8xlarge":16,"m4.10xlarge":8,"r3.8xlarge":8,"r4.4xlarge":4,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"m4.xlarge":0.75,"r4.8xlarge":8,"r4.large":0.5,"Standard_DS12":1,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"i3.large":0.75,"memory-optimized":1,"m4.large":0.375,"p2.16xlarge":24,"i3.8xlarge":8,"i3.16xlarge":16,"Standard_DS12_v2":1,"Standard_DS13":2,"Standard_DS11_v2":0.5,"Standard_DS13_v2":2,"c3.2xlarge":1,"Standard_L4s":1.5,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"i3.2xlarge":2,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"m4.16xlarge":12,"c4.8xlarge":4,"i3.xlarge":1,"r3.xlarge":1,"r4.2xlarge":2,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":false,"metastoreServiceRowLimit":1000000,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":true,"enableClusterTagsUI":false,"enableNotebookHistoryDiffing":true,"branch":"2.55","accountsLimit":3,"enableSparkEnvironmentVariables":true,"enableX509Authentication":false,"useAADLogin":false,"enableStructuredStreamingNbOptimizations":true,"enableNotebookGitBranching":true,"local":false,"enableNotebookLazyRenderWrapper":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"showReleaseNote":true,"displayDefaultContainerMemoryGB":6,"enableNotebookCommandMode":true,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"removePasswordInAccountSettings":false,"preferStartTerminatedCluster":false,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"sandboxForUrlSandboxFrame":"allow-scripts allow-popups allow-popups-to-escape-sandbox allow-forms","enableCssTransitions":true,"serverlessEnableElasticDisk":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":false,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableSshKeyUIByTier":false,"enableCreateClusterOnAttach":true,"defaultAutomatedPricePerDBU":0.2,"enableNotebookGitVersioning":true,"defaultMinWorkers":2,"files":"files/","feedbackEmail":"feedback@databricks.com","enableDriverLogsUI":true,"defaultMaxWorkers":8,"enableWorkspaceAclsConfig":false,"dropzoneMaxFileSize":2047,"enableNewClustersList":true,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"enableSparkEnvironmentVariablesUI":false,"defaultSparkVersion":{"key":"3.2.x-scala2.11","displayName":"3.2 (includes Apache Spark 2.2.0, Scala 2.11)","packageLabel":"spark-image-aca37a44b05c9c0177ca8a639ad186ebace4aac1a7bdcf73829945ab47414f41","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":false,"enableMountAclsConfig":false,"defaultAutoterminationMin":180,"useDevTierHomePage":true,"enableClusterClone":true,"enableNotebookLineNumbers":true,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":false,"enableNotebookDatasetInfoView":true,"enableClusterAclsByTier":false,"databricksDocsBaseUrl":"https://docs.databricks.com/","azurePortalLink":"https://portal.azure.com","cloud":"AWS","disallowAddingAdmins":true,"enableSparkConfUI":true,"featureTier":"DEVELOPER_BASIC_TIER","mavenCentralSearchEndpoint":"http://search.maven.org/solrsearch/select","enableOrgSwitcherUI":true,"bitbucketCloudBaseApiV2Url":"https://api.bitbucket.org/2.0","clustersLimit":1,"enableJdbcImport":true,"enableElasticDisk":false,"logfiles":"logfiles/","enableRelativeNotebookLinks":true,"enableMultiSelect":true,"enableWebappSharding":true,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"separateTableForJobClusters":true,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"dbcFeedbackURL":"mailto:feedback@databricks.com","enableMountAclService":true,"enableStructuredDataAcls":false,"showVersion":true,"serverlessClustersByDefault":false,"enableWorkspaceAcls":false,"maxClusterTagKeyLength":127,"gitHash":"7e487e8906bd141e3032c363a823d3c48d1535b5","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","serverlessAttachEbsVolumesByDefault":false,"enableTokensConfig":false,"allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableTokens":false,"enableMiniClusters":true,"enableNewJobList":true,"enableDebugUI":false,"enableStreamingMetricsDashboard":true,"allowNonAdminUsers":true,"enableSingleSignOnByTier":false,"enableJobsRetryOnTimeout":true,"useStandardTierUpgradeTooltips":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSpotClusterType":true,"enableSparkPackages":true,"checkAadUserInWorkspaceTenant":false,"dynamicSparkVersions":true,"enableClusterTagsUIByTier":false,"enableNotebookHistoryUI":true,"enableClusterLoggingUI":true,"enableDatabaseDropdownInTableUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":false,"enableFolderHtmlExport":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.databricks.com/_static/notebooks/gentle-introduction-to-apache-spark.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":"img/home/Python_icon.svg"}],"enableClusterStart":false,"enableEBSVolumesUIByTier":false,"singleSignOnComingSoon":false,"removeSubCommandCodeWhenExport":true,"upgradeURL":"https://accounts.cloud.databricks.com/registration.html#login","maxAutoterminationMinutes":10000,"autoterminateClustersByDefault":true,"notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":false,"showForgotPasswordLink":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"minAutoterminationMinutes":10,"accounts":true,"useOnDemandClustersByDefault":true,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"enableAutoCreateUserUI":true,"defaultCoresPerContainer":4,"showTerminationReason":true,"enableNewClustersGet":true,"showPricePerDBU":false,"showSqlProxyUI":true,"enableNotebookErrorHighlighting":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":2044923475617497,"name":"Streaming / Yahoo Benchmark / Flink","language":"scala","commands":[{"version":"CommandV1","origId":197135932734891,"guid":"a7228aea-9362-4ded-95d9-f39fa60655b6","subtype":"command","commandType":"auto","position":0.125,"command":"%md # Apache Flink\n\nMethods and declarations required for running the Yahoo Benchmark with Apache Flink.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f8f7f89c-61b7-4afe-98bb-84087b0447c8"},{"version":"CommandV1","origId":2044923475617499,"guid":"3734bebb-ec02-4de3-b7df-5d58405b1b22","subtype":"command","commandType":"auto","position":0.25,"command":"%run ./Utils","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":1497635644793,"submitTime":1497635644793,"finishTime":1497635661581,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"20a8a7ab-6ed2-4b11-b093-b724901302d1"},{"version":"CommandV1","origId":3497233294519194,"guid":"9ef39f10-a8db-4667-b013-d0fb9951f90c","subtype":"script","commandType":"auto","position":0.35,"command":"","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Warning: classes defined within packages cannot be redefined without a cluster restart.\nCompilation successful.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;driver&gt;:54: error: not found: value numExecutors\n    sc.parallelize(0 until numExecutors, numExecutors).foreach { i =&gt;\n                           ^\n&lt;driver&gt;:54: error: not found: value numExecutors\n    sc.parallelize(0 until numExecutors, numExecutors).foreach { i =&gt;\n                                         ^\nCompilation failed.</div>","error":null,"workflows":[],"startTime":1497635651884,"submitTime":1497563459163,"finishTime":1497635657300,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["3734bebb-ec02-4de3-b7df-5d58405b1b22"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6e29f404-c20d-4460-95fa-9fc487709dbc"},{"version":"CommandV1","origId":3497233294519195,"guid":"1cc12ced-3135-40fb-923a-8b27839fbb9b","subtype":"script","commandType":"auto","position":0.45,"command":"","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Warning: classes defined within packages cannot be redefined without a cluster restart.\nCompilation successful.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:52: error: not found: type SSHUtils\n    kafkaVersion: String = &quot;0.10.2.1&quot;) extends Serializable with SSHUtils {\n                                                                 ^\n&lt;console&gt;:83: error: not found: value generateSshKeys\n    generateSshKeys()\n    ^\n&lt;console&gt;:93: error: not found: value setupSSH\n    setupSSH(numExecutors)\n    ^\n&lt;console&gt;:96: error: not found: value ssh\n      ssh(ip, s&quot;bash /dbfs/$dbfsDir/install-kafka.sh&quot;)\n      ^\n&lt;console&gt;:99: error: not found: value ssh\n    ssh(zookeeper, s&quot;kafka/bin/zookeeper-server-start.sh -daemon kafka/config/zookeeper.properties&quot;)\n    ^\n&lt;console&gt;:102: error: not found: value ssh\n      ssh(host, s&quot;bash /dbfs/$dbfsDir/configure-kafka.sh $zookeeper $id $host&quot;)\n      ^\n&lt;console&gt;:103: error: not found: value ssh\n      ssh(host, s&quot;kafka/bin/kafka-server-start.sh -daemon kafka/config/server.properties&quot;)\n      ^\n&lt;console&gt;:110: error: not found: value ssh\n        ssh(ip, s&quot;sudo monit stop spark-slave&quot;)\n        ^\n&lt;console&gt;:116: error: not found: value ssh\n    ssh(kafkaNodes(0), s&quot;kafka/bin/kafka-topics.sh --create --topic $topic --partitions $partitions &quot; +\n    ^\n&lt;console&gt;:122: error: not found: value ssh\n      ssh(kafkaNodes(0), s&quot;kafka/bin/kafka-topics.sh --delete --topic $topic --zookeeper $zookeeper:2181&quot;)\n      ^\n</div>","error":null,"workflows":[],"startTime":1497635657309,"submitTime":1497563500345,"finishTime":1497635658195,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["3734bebb-ec02-4de3-b7df-5d58405b1b22"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2a6ca8c0-b730-4116-9c6b-a97dd2091f5c"},{"version":"CommandV1","origId":2044923475617502,"guid":"1d14febd-0b00-4592-98bd-8beaa7dc8330","subtype":"command","commandType":"auto","position":0.5,"command":"package com.databricks.benchmark.flink\n\nimport java.io.IOException\nimport java.util.Properties\n\nimport spray.json._\nimport org.apache.flink.api.common.functions.{FlatMapFunction, MapFunction}\nimport org.apache.flink.api.java.tuple.Tuple4\nimport org.apache.flink.api.scala._\nimport org.apache.flink.streaming.api.TimeCharacteristic\nimport org.apache.flink.streaming.api.operators.{AbstractUdfStreamOperator, ChainingStrategy, OneInputStreamOperator}\nimport org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}\nimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer010.FlinkKafkaProducer010Configuration\nimport org.apache.flink.streaming.connectors.kafka.{FlinkKafkaConsumer010, FlinkKafkaProducer010}\nimport org.apache.flink.streaming.runtime.streamrecord.StreamRecord\nimport org.apache.flink.streaming.util.serialization.{JSONKeyValueDeserializationSchema, SimpleStringSchema}\nimport org.apache.flink.util.Collector\n\nimport com.databricks.benchmark.yahoo.Event\n\n/**\n * Class used to aggregate windowed counts.\n * @param lastUpdate Event time of the last record received for a given `campaign_id` and `time_window`\n */\ncase class WindowedCount(\n    time_window: java.sql.Timestamp,\n    campaign_id: String,\n    var count: Long,\n    var lastUpdate: java.sql.Timestamp)\n\n/**\n * Json protocol defined for Spray when writing out aggregation results to Kafka.\n */\nobject WindowedCountJsonProtocol extends DefaultJsonProtocol {\n  implicit object WindowedCountJsonFormat extends RootJsonFormat[WindowedCount] {\n    def write(c: WindowedCount) = JsObject(\n      \"time_window\" -> JsNumber(c.time_window.getTime),\n      \"campaign_id\" -> JsString(c.campaign_id),\n      \"count\" -> JsNumber(c.count),\n      \"lastUpdate\" -> JsNumber(c.lastUpdate.getTime)\n    )\n    def read(value: JsValue): WindowedCount = {\n      new WindowedCount(null, \"\", 0, null) // we don't need to deserialize JSON in Flink\n    }\n  }\n}\n\nobject FlinkBenchmarkUtils {\n  \n  import WindowedCountJsonProtocol._\n\n  /** Standard Flink-provided kafka consumer using a json KV deserialization schema. */\n  def getKafkaConsumer(topic: String, properties: Properties): FlinkKafkaConsumer010[String] = {\n\n    properties.put(\"auto.offset.reset\", \"earliest\")\n\n    new FlinkKafkaConsumer010[String](topic, new SimpleStringSchema(), properties)\n  }\n\n  /** Flink Source that reads from Kafka. */\n  def getKafkaJsonStream(env: StreamExecutionEnvironment, topic: String, props: Properties): DataStream[String] = {\n    return env.addSource(getKafkaConsumer(topic, props))\n  }\n\n  /** Handle configuration of env here */\n  def getExecutionEnvironment(parallelism: Int = Int.MinValue): StreamExecutionEnvironment = {\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n    if (parallelism > 0) env.setParallelism(parallelism)\n    return env\n  }\n\n  /** Write strings to Kafka to the given topic */\n  def writeStringStream(inputData: DataStream[String], topic: String, props: Properties): FlinkKafkaProducer010Configuration[String] = {\n    FlinkKafkaProducer010.writeToKafkaWithTimestamps(inputData.javaStream, topic, new SimpleStringSchema, props)\n  }\n\n  /** Write an object as json to Kafka to the given topic */\n  def writeJsonStream(inputData: DataStream[WindowedCount], topic: String, props: Properties): FlinkKafkaProducer010Configuration[String] = {\n    FlinkKafkaProducer010.writeToKafkaWithTimestamps(\n      inputData.map(_.toJson.compactPrint).javaStream,\n      topic,\n      new SimpleStringSchema,\n      props)\n  }\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Warning: classes defined within packages cannot be redefined without a cluster restart.\nCompilation successful.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;driver&gt;:37: error: type mismatch;\n found   : Int(0)\n required: java.sql.Timestamp\n      new WindowedCount(0, &quot;&quot;, 0, 0) // we don't need to deserialize JSON in Flink\n                        ^\n&lt;driver&gt;:37: error: type mismatch;\n found   : Int(0)\n required: java.sql.Timestamp\n      new WindowedCount(0, &quot;&quot;, 0, 0) // we don't need to deserialize JSON in Flink\n                                  ^\nCompilation failed.</div>","error":null,"workflows":[],"startTime":1497560463724,"submitTime":1497560476829,"finishTime":1497560464981,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f03444c4-d6ca-49ba-aa22-6777ccd7f4ea"},{"version":"CommandV1","origId":3497233294519196,"guid":"7fecc56f-475c-4c8b-aacd-60af2ae14edf","subtype":"script","commandType":"auto","position":0.55,"command":"","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Warning: classes defined within packages cannot be redefined without a cluster restart.\nCompilation successful.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;driver&gt;:43: error: value length is not a member of scala.collection.immutable.Set[String]\n    if (executors.length == 1) {\n                  ^\nCompilation failed.</div>","error":null,"workflows":[],"startTime":1497635658201,"submitTime":1497563507562,"finishTime":1497635659050,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["3734bebb-ec02-4de3-b7df-5d58405b1b22"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"982cd07a-52f2-479d-a42c-dfdc2a23ced4"},{"version":"CommandV1","origId":2044923475617504,"guid":"e31e7d83-e8a8-4478-956d-a231521db654","subtype":"command","commandType":"auto","position":0.625,"command":"package com.databricks.benchmark.flink\n\nimport java.util.UUID\n\nimport org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction\nimport org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext\n\nimport com.databricks.benchmark.yahoo._\n\n/**\n * Our event generator for Flink.\n * @param tuplesPerSecond Target rate of event generation\n * @param rampUpTimeSeconds Time before we start generating events at full speed. During the first `rampUpTimeSeconds`, data is generated\n *                          at a linearly increasing rate. This allows for the JVM to warm up and allow systems to perform at higher speeds.\n * @param campaigns The ad campaigns to generate events for\n */\nclass EventGenerator(\n    tuplesPerSecond: Int,\n    rampUpTimeSeconds: Int,\n    campaigns: Seq[CampaignAd]) extends RichParallelSourceFunction[Event] {\n  \n  var running = true\n  private val uuid = UUID.randomUUID().toString // used as a dummy value for all events, based on ref code\n\n  private val adTypeLength = Variables.AD_TYPES.length\n  private val eventTypeLength = Variables.EVENT_TYPES.length\n  private val campaignLength = campaigns.length\n  private lazy val parallelism = getRuntimeContext().getNumberOfParallelSubtasks()\n\n  private def generateElement(i: Long, currentTime: Long): Event = {\n    val ad_id = campaigns(i % campaignLength toInt).ad_id // ad id for the current event index\n    val ad_type = Variables.AD_TYPES(i % adTypeLength toInt) // current adtype for event index\n    val event_type = Variables.EVENT_TYPES(i % eventTypeLength toInt) // current event type for event index\n    Event(\n      uuid, // random user, irrelevant\n      uuid, // random page, irrelevant\n      ad_id,\n      ad_type,\n      event_type,\n      new java.sql.Timestamp(currentTime),\n      \"255.255.255.255\") // generic ipaddress, irrelevant\n  }\n  \n  override def run(sourceContext: SourceContext[Event]): Unit = {\n    var start = 0L\n    val startTime = System.currentTimeMillis()\n\n    while (running) {\n      val emitStartTime = System.currentTimeMillis()\n      val elements = loadPerNode(startTime, emitStartTime)\n      var i = 0\n      while (i < elements) {\n        // We generate batches of elements with the same event timestamp, otherwise getting the timestamp becomes very expensive\n        sourceContext.collect(generateElement((start + i) % 10000, emitStartTime))\n        i += 1\n      }\n      start += elements\n      // Sleep for the rest of timeslice if needed\n      val emitTime = System.currentTimeMillis() - emitStartTime\n      if (emitTime < 1000) {\n        Thread.sleep(1000 - emitTime);\n      }\n    }\n    sourceContext.close()\n  }\n\n  override def cancel(): Unit = {\n    running = false\n  }\n\n\n  /**\n   * The amount of records to produce for a second given the parallelism, ramp up time and current time.\n   */\n  private def loadPerNode(startTime: Long, currentTime: Long): Int = {\n    val timeSinceStart = (currentTime - startTime) / 1000\n    if (timeSinceStart < rampUpTimeSeconds) {\n      Math.rint(timeSinceStart * 1.0 / rampUpTimeSeconds * tuplesPerSecond / parallelism).toInt\n    } else {\n      tuplesPerSecond / parallelism\n    }\n  }\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Warning: classes defined within packages cannot be redefined without a cluster restart.\nCompilation successful.\nwarning: there were three feature warnings; re-run with -feature for details\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;driver&gt;:41: error: missing argument list for method generateElement in class EventGenerator\nUnapplied methods are only converted to functions when a function type is expected.\nYou can make this conversion explicit by writing `generateElement _` or `generateElement(_)` instead of `generateElement`.\n        sourceContext.collect(generateElement);\n                              ^\nCompilation failed.</div>","error":null,"workflows":[],"startTime":1497560468197,"submitTime":1497560481304,"finishTime":1497560468705,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"03410bcd-d237-46ea-b11b-6b521baad68e"},{"version":"CommandV1","origId":3497233294519197,"guid":"a0c914ee-4fef-4c4a-8a3f-96cf5bf10759","subtype":"script","commandType":"auto","position":0.65,"command":"","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Warning: classes defined within packages cannot be redefined without a cluster restart.\nCompilation successful.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;driver&gt;:14: error: type mismatch;\n found   : _$1 where type _$1\n required: T\n  def parse[T: ClassTag](json: String): T = parser.readValue(json, classTag[T].runtimeClass)\n                                                            ^\nCompilation failed.</div>","error":null,"workflows":[],"startTime":1497635659054,"submitTime":1496441711822,"finishTime":1497635659381,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["3734bebb-ec02-4de3-b7df-5d58405b1b22"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d0ba08c0-c9bb-4281-984a-8364a891cdc2"},{"version":"CommandV1","origId":2044923475617506,"guid":"023bdece-239d-4386-8204-62c58b47cbb1","subtype":"command","commandType":"auto","position":0.75,"command":"package com.databricks.benchmark.flink\n\nimport java.sql.Timestamp\nimport java.util.{Properties, UUID}\n\nimport scala.io.Source\n\nimport com.fasterxml.jackson.databind.node.ObjectNode\nimport org.apache.flink.api.common.functions.FlatMapFunction\nimport org.apache.flink.api.common.state.ValueState\nimport org.apache.flink.api.common.typeinfo.TypeInformation\nimport org.apache.flink.api.java.tuple.Tuple\nimport org.apache.flink.api.java.utils.ParameterTool\nimport org.apache.flink.streaming.api.functions.TimestampExtractor\nimport org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor\nimport org.apache.flink.streaming.api.scala._\nimport org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows\nimport org.apache.flink.streaming.api.windowing.time.Time\nimport org.apache.flink.streaming.api.windowing.triggers.Trigger\nimport org.apache.flink.streaming.api.windowing.triggers.Trigger._\nimport org.apache.flink.streaming.api.windowing.triggers.TriggerResult\nimport org.apache.flink.streaming.api.windowing.windows.TimeWindow\nimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer010\nimport org.apache.flink.util.Collector\n\nimport com.databricks.benchmark.utils.Json\nimport com.databricks.benchmark.yahoo._\n\nobject YahooBenchmark {\n  \n  // Transcribed from https://github.com/dataArtisans/yahoo-streaming-benchmark/blob/d8381f473ab0b72e33469d2b98ed1b77317fe96d/flink-benchmarks/src/main/java/flink/benchmark/AdvertisingTopologyFlinkWindows.java#L179\n  class EventAndProcessingTimeTrigger(triggerIntervalMs: Int) extends Trigger[Any, TimeWindow] {\n    \n    var nextTimer: Long = 0L\n    \n    override def onElement(element: Any, timestamp: Long, window: TimeWindow, ctx: TriggerContext): TriggerResult = {\n      ctx.registerEventTimeTimer(window.maxTimestamp())\n      // register system timer only for the first time\n      val firstTimerSet = ctx.getKeyValueState(\"firstTimerSet\", classOf[java.lang.Boolean], new java.lang.Boolean(false))\n      if (!firstTimerSet.value()) {\n        nextTimer = System.currentTimeMillis() + triggerIntervalMs\n        ctx.registerProcessingTimeTimer(nextTimer)\n        firstTimerSet.update(true)\n      }\n      return TriggerResult.CONTINUE\n    }\n\n    override def onEventTime(time: Long, window: TimeWindow, ctx: TriggerContext): TriggerResult = {\n      TriggerResult.FIRE_AND_PURGE\n    }\n\n    override def onProcessingTime(time: Long, window: TimeWindow, ctx: TriggerContext): TriggerResult = {\n      // schedule next timer\n      nextTimer = System.currentTimeMillis() + triggerIntervalMs\n      ctx.registerProcessingTimeTimer(nextTimer)\n      TriggerResult.FIRE;\n    }\n    \n    override def clear(window: TimeWindow, ctx: TriggerContext): Unit = {\n      ctx.deleteProcessingTimeTimer(nextTimer)\n      ctx.deleteEventTimeTimer(window.maxTimestamp())\n\t}\n  }\n  \n  /**\n   * A logger that prints out the number of records processed and the timestamp, which we can later use for throughput calculation.\n   */\n  class ThroughputLogger(logFreq: Long) extends FlatMapFunction[Event, Integer] {\n    private var totalReceived: Long = 0\n    \n    override def flatMap(element: Event, collector: Collector[Integer]): Unit = {\n      if (totalReceived == 0) {\n        println(s\"ThroughputLogging:${System.currentTimeMillis()},${totalReceived}\")\n      }\n      totalReceived += 1\n      if (totalReceived % logFreq == 0) {\n        println(s\"ThroughputLogging:${System.currentTimeMillis()},${totalReceived}\")\n      }\n    }\n  }\n  \n  class StaticJoinMapper(campaigns: Map[String, String]) extends FlatMapFunction[Event, (String, String, Timestamp)] {\n    override def flatMap(element: Event, collector: Collector[(String, String, Timestamp)]): Unit = {\n      collector.collect((campaigns(element.ad_id), element.ad_id, element.event_time))\n    }\n  }\n  \n  class AdTimestampExtractor extends TimestampExtractor[(String, String, Timestamp)] {\n\n    var maxTimestampSeen: Long = 0L\n\n    override def extractTimestamp(element: (String, String, Timestamp), currentTimestamp: Long): Long = {\n      val timestamp = element._3.getTime\n      maxTimestampSeen = Math.max(timestamp, maxTimestampSeen)\n      timestamp\n    }\n\n    override def extractWatermark(element: (String, String, Timestamp), currentTimestamp: Long) = Long.MinValue\n\n    override def getCurrentWatermark(): Long = maxTimestampSeen - 1L\n  }\n  \n  def main(args: Array[String]): Unit = {\n    val params: ParameterTool = ParameterTool.fromArgs(args)\n\n    val windowMillis: Time = Time.milliseconds(params.getLong(\"windowMillis\", 10000))\n    val parallelism: Int = params.getInt(\"parallelism\", 5)\n    require(parallelism > 0, \"Parallelism needs to be a positive integer.\")\n    // Used for assigning event times from out of order data\n\n    // Used when generating input\n    val numCampaigns: Int = params.getInt(\"numCampaigns\", 100)\n    val tuplesPerSecond: Int = params.getInt(\"tuplesPerSecond\", 50000)\n    val rampUpTimeSeconds: Int = params.getInt(\"rampUpTimeSeconds\", 0)\n    val triggerIntervalMs: Int = params.getInt(\"triggerIntervalMs\", 0)\n    require(triggerIntervalMs >= 0, \"Trigger interval can't be negative.\")\n    // Logging frequency in #records for throughput calculations\n    val logFreq: Int = params.getInt(\"logFreq\", 10000000)\n    val outputTopic: String = params.get(\"outputTopic\", \"YahooBenchmarkOutput\")\n\n    val props = params.getProperties\n\n    val env: StreamExecutionEnvironment = FlinkBenchmarkUtils.getExecutionEnvironment(parallelism)\n\n    val campaignAdSeq: Seq[CampaignAd] = generateCampaignMapping(numCampaigns) \n  \n    // According to https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/StreamingRuntimeContext.java\n    // it looks like broadcast variables aren't actually supported in streaming...\n    // curious how others are achieving a join against a static set. For now we simply include the lookup map in the closure.\n    val campaignLookup: Map[String, String] = campaignAdSeq.map(ca => (ca.ad_id, ca.campaign_id)).toMap\n\n    val events: DataStream[Event] = env.addSource(new EventGenerator(tuplesPerSecond, rampUpTimeSeconds, campaignAdSeq))\n    \n    events.flatMap(new ThroughputLogger(logFreq))\n\n    val windowedEvents: WindowedStream[(String, String, Timestamp), Tuple, TimeWindow] = events\n      .filter(_.event_type == \"view\")\n      .flatMap(new StaticJoinMapper(campaignLookup))\n      .assignTimestamps(new AdTimestampExtractor())\n      .keyBy(0) // campaign_id\n      .window(TumblingEventTimeWindows.of(windowMillis))\n    \n    // set a trigger to reduce latency. Leave it out to increase throughput\n    if (triggerIntervalMs > 0) {\n      windowedEvents.trigger(new EventAndProcessingTimeTrigger(triggerIntervalMs))\n    }\n\n    val windowedCounts = windowedEvents.fold(new WindowedCount(null, \"\", 0, new java.sql.Timestamp(0L)),\n      (acc: WindowedCount, r: (String, String, Timestamp)) => {\n        val lastUpdate = if (acc.lastUpdate.getTime < r._3.getTime) r._3 else acc.lastUpdate\n        acc.count += 1\n        acc.lastUpdate = lastUpdate\n        acc\n      },\n      (key: Tuple, window: TimeWindow, input: Iterable[WindowedCount], out: Collector[WindowedCount]) => {\n        val windowedCount = input.iterator.next()\n        out.collect(new WindowedCount(\n          new java.sql.Timestamp(window.getStart), key.getField(0), windowedCount.count, windowedCount.lastUpdate))\n      }\n    )\n\n    FlinkBenchmarkUtils.writeJsonStream(windowedCounts, outputTopic, props)\n\n    env.execute(\"Flink Yahoo Benchmark\")\n  }\n\n  /** Generate in-memory ad_id to campaign_id map. We generate 10 ads per campaign. */\n  private def generateCampaignMapping(numCampaigns: Int): Seq[CampaignAd] = {\n    Seq.tabulate(numCampaigns) { _ => \n      val campaign = UUID.randomUUID().toString\n      Seq.tabulate(10)(_ => CampaignAd(UUID.randomUUID().toString, campaign))\n    }.flatten\n  }\n}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Warning: classes defined within packages cannot be redefined without a cluster restart.\nCompilation successful.\n&lt;driver&gt;:39: warning: method getKeyValueState in trait TriggerContext is deprecated: see corresponding Javadoc for more information.\n      val firstTimerSet = ctx.getKeyValueState(&quot;firstTimerSet&quot;, classOf[java.lang.Boolean], new java.lang.Boolean(false))\n                              ^\nwarning: there were three feature warnings; re-run with -feature for details\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;driver&gt;:39: warning: method getKeyValueState in trait TriggerContext is deprecated: see corresponding Javadoc for more information.\n      val firstTimerSet = ctx.getKeyValueState(&quot;firstTimerSet&quot;, classOf[java.lang.Boolean], new java.lang.Boolean(false))\n                              ^\n&lt;driver&gt;:65: error: class ThroughputLogger needs to be abstract, since method flatMap in trait FlatMapFunction of type (x$1: com.databricks.benchmark.yahoo.Event, x$2: org.apache.flink.util.Collector[Integer])Unit is not defined\n  class ThroughputLogger(logFreq: Long) extends FlatMapFunction[Event, Integer] {\n        ^\n&lt;driver&gt;:68: error: method flatMap overrides nothing.\nNote: the super classes of class ThroughputLogger contain the following, non final members named flatMap:\ndef flatMap(x$1: com.databricks.benchmark.yahoo.Event,x$2: org.apache.flink.util.Collector[Integer]): Unit\n    override def flatMap(element: Any, collector: Collector[Integer]): Unit = {\n                 ^\nCompilation failed.</div>","error":null,"workflows":[],"startTime":1497560625936,"submitTime":1497560639042,"finishTime":1497560627374,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"dc145070-991b-45e0-b695-691568866188"},{"version":"CommandV1","origId":3497233294519198,"guid":"91835fa2-0df5-4864-99ab-02e0d91e5aac","subtype":"script","commandType":"auto","position":0.75,"command":"","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Warning: classes defined within packages cannot be redefined without a cluster restart.\nCompilation successful.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1497635659385,"submitTime":1496441714347,"finishTime":1497635659842,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["3734bebb-ec02-4de3-b7df-5d58405b1b22"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d02e8eb0-6c1d-4263-b55b-53e42d7bd4d5"},{"version":"CommandV1","origId":3497233294519199,"guid":"8c69b9b9-45d4-4c03-a4e0-edb5f7209190","subtype":"script","commandType":"auto","position":0.85,"command":"","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Warning: classes defined within packages cannot be redefined without a cluster restart.\nCompilation successful.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1497635659845,"submitTime":1496437832537,"finishTime":1497635660315,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["3734bebb-ec02-4de3-b7df-5d58405b1b22"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fb9def09-0523-4341-bedb-f974a8c1ca01"},{"version":"CommandV1","origId":2044923475617508,"guid":"d5c7d61b-53cd-48c9-ba8a-9ea029a3b396","subtype":"command","commandType":"auto","position":0.875,"command":"import com.databricks.spark.{LocalKafka, LocalFlink}\n\n/**\n * Runner for the Yahoo Benchmark using Apache Flink.\n * @param parallelism The parallelism to run the Yahoo Benchmark at. Needs to be smaller than or equal to\n *                    `flinkCluster.numTaskSlots * flinkCluster.numFlinkNodes`.\n * @param triggerIntervalMs How often to emit updates for the windowed counts\n * @param logFreq The logging frequency for throughput. Lower values will give more accurate throughput calculations\n *                but may sacrifice throughput in the process.\n */\nclass FlinkYahooRunner(\n    override val spark: SparkSession,\n    flinkCluster: LocalFlink,\n    kafkaCluster: LocalKafka,\n    parallelism: Int,\n    triggerIntervalMs: Int,\n    logFreq: Int = 10000000) extends YahooBenchmarkRunner {\n  \n  import scala.sys.process._\n  \n  require(parallelism >= 1, \"Parallelism can't be less than 1\")\n  require(triggerIntervalMs >= 0, \"The trigger interval needs to be greater than or equal to 0\")\n  require(logFreq >= 1, \"The logging frequency needs to be greater than 0\")\n  \n  override def start(): Unit = {\n    Thread.sleep(1000000000L)\n  }\n  \n  override def generateData(\n      campaigns: Array[CampaignAd],\n      tuplesPerSecond: Long,\n      recordGenParallelism: Int,\n      rampUpTimeSeconds: Int): Unit = {\n    flinkCluster.runJob(\"com.databricks.benchmark.flink.YahooBenchmark\",\n      \"--bootstrap.servers\", kafkaCluster.kafkaNodesString,\n      \"--parallelism\", parallelism.toString,\n      \"--tuplesPerSecond\", tuplesPerSecond.toString,\n      \"--numCampaigns\", campaigns.length.toString,\n      \"--rampUpTimeSeconds\", rampUpTimeSeconds.toString,\n      \"--triggerIntervalMs\", triggerIntervalMs.toString,\n      \"--logFreq\", logFreq.toString,\n      \"--outputTopic\", Variables.OUTPUT_TOPIC)\n  }\n  \n  override def stop(): Unit = {\n    val jobs = \"flink/bin/flink list -r\".!!.split(\"\\n\").find(_.contains(\"Flink Yahoo Benchmark\"))\n    if (jobs.isDefined) {\n      s\"flink/bin/flink cancel ${jobs.get.split(\" : \")(1).trim}\".!!\n    }\n  }\n  \n  override val params: Map[String, Any] = Map(\"parallelism\" -> parallelism, \"triggerIntervalMs\" -> triggerIntervalMs)\n  \n  private val LOG_FORMAT = \"ThroughputLogging:(\\\\d*),(\\\\d*)\".r\n\n  private case class PerNodeStats(start: Long, end: Long, count: Long)  \n  \n  /**\n   * Throughput calculation in Flink works as follows:\n   *   1. The `ThroughputLogger` periodically logs the number of records per task and the current time to the taskmanager stdout logs\n   *   2. We iterate through the stdout logs to find the last run. The zeros mark the start the start of a new run. The number of tasks\n   *      is equal to the number of zeros we observe for this run.\n   *   3. We then take the last timestamp and highest number of records processed for that run, and multiply by the number of tasks to\n   *      get the number of records processed.\n   *   4. We merge these numbers across Flink taskmanagers to get the total number of records processed, earliest timestamp (job start),\n   *      and latest timestamp (job end) to calculate the throughput.\n   *\n   * The throughput calculation is therefore an approximation and the error on the number of records processed\n   * is strictly less than `logFreq * parallelism`.\n   */\n  override def getThroughput(): DataFrame = {\n    val perNode = flinkCluster.taskManagers.map { node =>\n      val log = if (flinkCluster.isJobManagerOnDriver) {\n        val fileName = \"ls flink/log/\".!!.split(\"\\n\").find(_.endsWith(\".out\")).get\n        s\"cat flink/log/$fileName\".!!.split(\"\\n\")\n      } else {\n        flinkCluster.ssh(node, \"cat flink/log/flink-ubuntu-taskmanager-*.out\", logStdout = false).split(\"\\n\")\n      }\n      var highestSeen = 0L\n      var latestTimestamp = 0L\n      var earliestTimestamp = -1L\n      var numTasks = 0\n      log.foreach { case LOG_FORMAT(timestampStr, countStr) =>\n        val timestamp = timestampStr.toLong\n        val count = countStr.toLong\n        if (count == 0) {\n          if (highestSeen > 0 || earliestTimestamp == -1L) {\n            // this is a new run\n            highestSeen = 0\n            earliestTimestamp = timestamp\n            numTasks = 1\n          } else if (highestSeen == 0) {\n            numTasks += 1\n          }\n        }\n        if (timestamp > latestTimestamp) {\n          latestTimestamp = timestamp\n        }\n        if (count > highestSeen) {\n          highestSeen = count\n        }\n      }\n      PerNodeStats(earliestTimestamp, latestTimestamp, highestSeen * numTasks)\n    }\n    val aggregated = perNode.reduce { (stats1, stats2) =>\n      PerNodeStats(Math.min(stats1.start, stats2.start), Math.max(stats1.end, stats2.end), stats1.count + stats2.count)\n    }\n    val duration = aggregated.end - aggregated.start\n    Seq((\n        new java.sql.Timestamp(aggregated.start),\n        new java.sql.Timestamp(aggregated.end),\n        aggregated.count,\n        duration,\n        aggregated.count * 1000.0 / duration)).toDS()\n      .toDF(\"start\", \"end\", \"totalInput\", \"totalDurationMillis\", \"throughput\")\n  }\n  \n  /**\n   * We calculate the latency as the difference between the Kafka ingestion timestamp for a given `time_window` and `campaign_id`\n   * pair and the event timestamp of the latest record generated that belongs to that bucket.\n   */\n  override def getLatency(): DataFrame = {\n    import org.apache.spark.sql.types._\n    val schema = YahooBenchmark.outputSchema.add(\"lastUpdate\", LongType)\n    val realTimeMs = udf((t: java.sql.Timestamp) => t.getTime)\n    \n    spark.read\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", kafkaCluster.kafkaNodesString)\n      .option(\"subscribe\", Variables.OUTPUT_TOPIC)\n      .load()\n      .withColumn(\"result\", from_json($\"value\".cast(\"string\"), schema))\n      .select(\n        $\"timestamp\" as 'resultOutput,\n        $\"result.*\")\n      .groupBy($\"time_window\", $\"campaign_id\")\n      .agg(max($\"resultOutput\") as 'resultOutput, max('lastUpdate) as 'lastUpdate)\n      .withColumn(\"diff\", realTimeMs($\"resultOutput\") - 'lastUpdate)\n      .selectExpr(\n        \"min(diff) as latency_min\",\n        \"mean(diff) as latency_avg\",\n        \"percentile_approx(diff, 0.95) as latency_95\",\n        \"percentile_approx(diff, 0.99) as latency_99\",\n        \"max(diff) as latency_max\")\n  }\n}","commandVersion":0,"state":"error","results":null,"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:44: error: not found: type YahooBenchmarkRunner\n           logFreq: Int = 10000000) extends YahooBenchmarkRunner {\n                                            ^\n&lt;console&gt;:39: error: not found: type SparkSession\n           override val spark: SparkSession,\n                               ^\n&lt;console&gt;:51: error: not found: type CampaignAd\n             campaigns: Array[CampaignAd],\n                              ^\n&lt;console&gt;:63: error: not found: value Variables\n             &quot;--outputTopic&quot;, Variables.OUTPUT_TOPIC)\n                              ^\n&lt;console&gt;:75: error: not found: type DataFrame\n         override def getThroughput(): DataFrame = {\n                                       ^\n&lt;console&gt;:79: error: not found: type DataFrame\n         override def getLatency(): DataFrame = {\n                                    ^\n&lt;console&gt;:81: error: not found: value YahooBenchmark\n           val schema = YahooBenchmark.outputSchema.add(&quot;lastUpdate&quot;, TimestampType)\n                        ^\n</div>","error":null,"workflows":[],"startTime":1497559771095,"submitTime":1497559771095,"finishTime":1497559773885,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"01cce013-954d-4a5f-852c-97f23e5248bd"},{"version":"CommandV1","origId":3497233294519200,"guid":"b069aadd-aa40-4ac7-a6bb-245259c8be82","subtype":"script","commandType":"auto","position":0.95,"command":"","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Warning: classes defined within packages cannot be redefined without a cluster restart.\nCompilation successful.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;driver&gt;:88: error: not found: value generateStream\n    generateStream(spark, campaigns, tuplesPerSecond, recordGenParallelism, rampUpTimeSeconds)\n    ^\nCompilation failed.</div>","error":null,"workflows":[],"startTime":1497635660319,"submitTime":1497036075966,"finishTime":1497635660762,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["3734bebb-ec02-4de3-b7df-5d58405b1b22"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"55be3962-38ab-48b0-88fe-e41cd7a44987"},{"version":"CommandV1","origId":3497233294519201,"guid":"8ae77780-b7c0-42f9-b671-53875d8a8f70","subtype":"script","commandType":"auto","position":1.05,"command":"","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import scala.collection.JavaConverters._\nimport org.apache.spark.sql.SparkSession\nimport com.databricks.benchmark.yahoo.YahooBenchmarkRunner\ndefined trait Benchmark\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:94: error: abstract member may not have private modifier\n         private def startReader(): Unit\n                     ^\n</div>","error":null,"workflows":[],"startTime":1497635660766,"submitTime":1496436911439,"finishTime":1497635661149,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["3734bebb-ec02-4de3-b7df-5d58405b1b22"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"1b0cbf94-0413-4f67-b76b-51702adad1a4"},{"version":"CommandV1","origId":3497233294519202,"guid":"8ee26fc0-6a42-4ffc-8d31-e6c622647466","subtype":"script","commandType":"auto","position":1.15,"command":"","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import java.util.UUID\nimport org.apache.spark.sql.{DataFrame, Encoders}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport com.databricks.spark.LocalKafka\nimport com.databricks.benchmark.yahoo._\ndefined class YahooBenchmark\ndefined object YahooBenchmark\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:90: error: not found: value CampaignAd\n           CampaignAd(UUID.randomUUID().toString, UUID.randomUUID().toString)\n           ^\n</div>","error":null,"workflows":[],"startTime":1497635661156,"submitTime":1496436926089,"finishTime":1497635661573,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["3734bebb-ec02-4de3-b7df-5d58405b1b22"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"7b6af53b-f8a6-4957-b99f-6e2529f3a3cc"}],"dashboards":[],"guid":"29ec9774-a56a-475a-b299-50af68101a5e","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":0,"guid":"4df9d663-1d9d-4554-bee7-63e0ea72e236","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{}},"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{}};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>